{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769a266d",
   "metadata": {},
   "source": [
    "\n",
    "# Azure OpenAI Latency Test — Manhattan Life (Two-Row Header Flattening)\n",
    "\n",
    "This notebook measures **LLM response time** for generating a **field-mapping rule-spec JSON** using your uploaded Manhattan Life CSV.\n",
    "\n",
    "**Key features:**\n",
    "- Uses your two-row header flattener to normalize headers from `/mnt/data/manhattan_life_raw_data.csv`.\n",
    "- Sends **flattened headers** + your **carrier prompt** to Azure OpenAI once per trial.\n",
    "- Reports mean/median/min/max/p95 latency and prints the last JSON rule-spec (truncated).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669a3ee",
   "metadata": {},
   "source": [
    "## 0) Environment — Azure OpenAI settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Set these to your actual values (or set them in your environment before running)\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"<YOUR_API_KEY>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://<your-resource>.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-4o-mini\")  # your chat deployment name\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8ba37",
   "metadata": {},
   "source": [
    "## 1) Utilities — Two-Row Header Flattener (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "## Load and flatten multi-row header\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str)\n",
    "    tmp = tmp.fillna(\"\")\n",
    "\n",
    "    # First two rows are headers\n",
    "    top = tmp.iloc[0].tolist()\n",
    "    bottom = tmp.iloc[1].tolist()\n",
    "\n",
    "    # Forward-fill across top header row\n",
    "    ff = []\n",
    "    last = \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    # Build merged column names\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "\n",
    "        # Normalize names (remove spaces, slashes, periods)\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    # Drop the two header rows, assign new columns\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "\n",
    "    # Drop empty columns\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31594e",
   "metadata": {},
   "source": [
    "## 2) Load & Inspect Flattened Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path(\"/mnt/data/manhattan_life_raw_data.csv\")\n",
    "if not RAW_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {RAW_PATH}\")\n",
    "\n",
    "df_flat = flatten_two_header_csv(RAW_PATH.as_posix())\n",
    "CSV_HEADERS = list(df_flat.columns)\n",
    "\n",
    "print(\"Detected flattened headers (count =\", len(CSV_HEADERS), \"):\\n\", CSV_HEADERS[:25], \"...\")\n",
    "print(\"Data preview:\")\n",
    "df_flat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938eff8",
   "metadata": {},
   "source": [
    "## 3) Required destination fields & (paste) Manhattan Life prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "# ⬇️ Paste your real Manhattan Life prompt text here\n",
    "RULES_TEXT = \"\"\"\n",
    "You are a data transformation agent...\n",
    "(Replace this with the exact text of carrier_prompts/manhattan_life_prompt.txt)\n",
    "If a rule is unclear or TBD, return blank for that field.\n",
    "Use 'PID' if the rule refers to PID but output needs PTD (executor will map PID→PTD).\n",
    "Return STRICT JSON only. No prose.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6505245",
   "metadata": {},
   "source": [
    "## 4) AzureChatOpenAI setup and latency benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, statistics, time\n",
    "\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import AzureChatOpenAI  # fallback\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Output JSON ONLY, no prose.\n",
    "Return a mapping where keys are required output fields and values are objects:\n",
    "{ \"op\": <one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,money,membercount_from_commission,blank]>, \n",
    "  \"source\": <column name when applicable>, \n",
    "  \"value\": <for const> }.\n",
    "If a rule says 'TBD' or 'blank' or is unclear, use {\"op\":\"blank\"}.\n",
    "Never invent columns. Use exact source header strings when copying.\n",
    "Use 'PID' if rules refer to PID but output needs PTD.\n",
    "\"\"\"\n",
    "\n",
    "def build_llm(timeout: int = 20, temperature: float = 1.0) -> AzureChatOpenAI:\n",
    "    # Note: some Azure deployments lock temperature to 1.0; pass 1.0 to avoid 400 errors.\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "    if not (api_key and endpoint and deployment):\n",
    "        raise RuntimeError(\"Missing AZURE_OPENAI_* env vars\")\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "        max_retries=0  # raw latency\n",
    "    )\n",
    "\n",
    "def time_rule_spec_build(headers, rules_text, trials: int = 5, timeout: int = 20, temperature: float = 1.0):\n",
    "    llm = build_llm(timeout=timeout, temperature=temperature)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": json.dumps({\n",
    "            \"RequiredFields\": FINAL_COLUMNS,\n",
    "            \"RawHeaders\": headers,\n",
    "            \"RulesNarrative\": rules_text,\n",
    "            \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "        }, ensure_ascii=False)}\n",
    "    ]\n",
    "    latencies, outputs = [], []\n",
    "\n",
    "    # Warmup (not counted)\n",
    "    try:\n",
    "        _ = llm.invoke(messages)\n",
    "    except Exception as e:\n",
    "        print(\"Warmup failed:\", e)\n",
    "\n",
    "    for i in range(trials):\n",
    "        t0 = time.perf_counter()\n",
    "        resp = llm.invoke(messages)\n",
    "        t1 = time.perf_counter()\n",
    "        latencies.append(t1 - t0)\n",
    "        outputs.append(resp.content if hasattr(resp, \"content\") else str(resp))\n",
    "\n",
    "    lat_sorted = sorted(latencies)\n",
    "    p95 = lat_sorted[int(0.95*(len(lat_sorted)-1))] if latencies else None\n",
    "    return {\n",
    "        \"trials\": trials,\n",
    "        \"latencies_s\": latencies,\n",
    "        \"mean_s\": statistics.fmean(latencies) if latencies else None,\n",
    "        \"median_s\": statistics.median(latencies) if latencies else None,\n",
    "        \"min_s\": min(latencies) if latencies else None,\n",
    "        \"max_s\": max(latencies) if latencies else None,\n",
    "        \"p95_s\": p95,\n",
    "        \"last_output\": outputs[-1] if outputs else \"\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c686c6c",
   "metadata": {},
   "source": [
    "## 5) Run the latency benchmark (using flattened headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = time_rule_spec_build(headers=CSV_HEADERS, rules_text=RULES_TEXT, trials=5, timeout=20, temperature=1.0)\n",
    "print(\"Latency (seconds):\")\n",
    "print({k: v for k, v in results.items() if k not in (\"latencies_s\",\"last_output\")})\n",
    "print(\"\\nRaw latencies per trial (s):\", results[\"latencies_s\"])\n",
    "print(\"\\nLast JSON output (truncated to 1200 chars):\\n\")\n",
    "print(results[\"last_output\"][:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def _needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\n",
    "        \"money\",\"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def audit_rule_spec(df: pd.DataFrame, rule_spec: dict, topk: int = 3):\n",
    "    headers = list(df.columns)\n",
    "    headers_norm = {re.sub(r'[^a-z0-9]', '', h.lower()): h for h in headers}\n",
    "    missing = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        if tgt == \"PID\":  # will map to PTD\n",
    "            pass\n",
    "        op = spec.get(\"op\",\"\")\n",
    "        if not _needs_source(op):\n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            missing.append((tgt, op, src, [], []))\n",
    "            continue\n",
    "        # exact (case-insensitive) check\n",
    "        if src in headers or src.lower() in [h.lower() for h in headers]:\n",
    "            continue\n",
    "        # normalized check\n",
    "        src_norm = re.sub(r'[^a-z0-9]', '', src.lower())\n",
    "        if src_norm in headers_norm:\n",
    "            continue\n",
    "        # fuzzy suggestions\n",
    "        candidates = get_close_matches(src, headers, n=topk) or []\n",
    "        candidates2 = get_close_matches(src_norm, [re.sub(r'[^a-z0-9]','',h.lower()) for h in headers], n=topk)\n",
    "        cand2_orig = []\n",
    "        for c2 in candidates2:\n",
    "            for h in headers:\n",
    "                if re.sub(r'[^a-z0-9]','',h.lower()) == c2:\n",
    "                    cand2_orig.append(h)\n",
    "        missing.append((tgt, op, src, candidates, cand2_orig))\n",
    "    return missing\n",
    "\n",
    "# After you have df_flat and rule_spec:\n",
    "missing = audit_rule_spec(df_flat, rule_spec)\n",
    "for tgt, op, src, c1, c2 in missing:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) source='{src}'\")\n",
    "    if c1: print(\"  close (raw):\", c1)\n",
    "    if c2: print(\"  close (norm):\", c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ffde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "def _build_header_index(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a robust index: normalized header -> original header.\n",
    "    Normalization: lowercase, remove non-alphanumerics.\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for h in df.columns:\n",
    "        key = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[key] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: Dict[str, str], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Case/punctuation/underscore-insensitive lookup.\n",
    "    Tries raw exact, case-insensitive, normalized match.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "    # raw exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    # case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    # normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    # give up\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d32fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Load rules from JSON (not inline), then compile\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "RULES_FILE = Path(\"carrier_prompts/manhattan_life_rules.json\")\n",
    "RULES_TEXT = RULES_FILE.read_text(encoding=\"utf-8\")  # keep the rules outside the code\n",
    "\n",
    "rule_spec = derive_rule_spec(\n",
    "    required_fields=FINAL_COLUMNS,   # actually use the arg\n",
    "    headers=list(df_flat.columns),\n",
    "    rules_text=RULES_TEXT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f90a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_rule_spec(required_fields, headers, rules_text) -> dict:\n",
    "    llm = build_llm(temperature=1.0)\n",
    "    payload = {\n",
    "        \"RequiredFields\": required_fields,   # <-- not FINAL_COLUMNS\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": rules_text,\n",
    "        \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "    ])\n",
    "    return json.loads(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Make header lookup robust (spaces vs underscores, case, punctuation)\n",
    "import re\n",
    "\n",
    "def _build_header_index(df):\n",
    "    # normalized header -> original header\n",
    "    idx = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df, header_index, name: str):\n",
    "    if not name:\n",
    "        return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    # exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    # case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    # normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    # not found\n",
    "    return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ad433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules_vectorized(df, rule_spec):\n",
    "    header_index = _build_header_index(df)\n",
    "    ...\n",
    "    # replace previous calls with:\n",
    "    src = _get_col(df, header_index, spec.get(\"source\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b281fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Audit mismatches before transforming\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def _needs_src(op:str) -> bool:\n",
    "    return op in {\"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "                  \"name_first_from_full\",\"name_last_from_full\",\n",
    "                  \"money\",\"membercount_from_commission\"}\n",
    "\n",
    "def audit_rule_spec(df, rule_spec, topk=3):\n",
    "    headers = list(df.columns)\n",
    "    norm = {re.sub(r'[^a-z0-9]','',h.lower()):h for h in headers}\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        op = spec.get(\"op\",\"\")\n",
    "        if not _needs_src(op): \n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            print(f\"[MISSING] {tgt} ({op}) source=''\")\n",
    "            continue\n",
    "        if src in headers or any(h.lower()==src.lower() for h in headers):\n",
    "            continue\n",
    "        key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "        if key in norm:\n",
    "            continue\n",
    "        c1 = get_close_matches(src, headers, n=topk)\n",
    "        c2 = get_close_matches(key, list(norm.keys()), n=topk)\n",
    "        c2 = [norm[k] for k in c2]\n",
    "        print(f\"[MISSING] {tgt} <- ({op}) source='{src}'\")\n",
    "        if c1: print(\"  close (raw):\", c1)\n",
    "        if c2: print(\"  close (norm):\", c2)\n",
    "\n",
    "audit_rule_spec(df_flat, rule_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Quick end-to-end (after you fix the rules file)\n",
    "# 1) Flatten\n",
    "df_flat = flatten_two_header_csv(\"/mnt/data/manhattan_life_raw_data.csv\")\n",
    "\n",
    "# 2) Build spec once\n",
    "RULES_TEXT = Path(\"carrier_prompts/manhattan_life_rules.json\").read_text(encoding=\"utf-8\")\n",
    "rule_spec = derive_rule_spec(FINAL_COLUMNS, list(df_flat.columns), RULES_TEXT)\n",
    "\n",
    "# 3) Audit (should print nothing if all aligned)\n",
    "audit_rule_spec(df_flat, rule_spec)\n",
    "\n",
    "# 4) Transform\n",
    "out_df = apply_rules_vectorized(df_flat, rule_spec)\n",
    "\n",
    "# 5) Save\n",
    "out_df.to_csv(\"/mnt/data/outbound/manhattan_life_standard.csv\", index=False)\n",
    "Path(\"/mnt/data/outbound/manhattan_life_standard.json\").write_text(\n",
    "    out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44332bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ---- Standard output schema (order preserved) ----\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "# ---------- Robust header lookup ----------\n",
    "def _build_header_index(df: pd.DataFrame) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a normalized header -> original header index.\n",
    "    Normalization: lowercase, strip all non [a-z0-9].\n",
    "    \"\"\"\n",
    "    idx: dict[str, str] = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: dict[str, str], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Case/underscore/punctuation insensitive column fetch.\n",
    "    Returns empty-string Series if not found.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    # 1) exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "\n",
    "    # 2) case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "\n",
    "    # 3) normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "\n",
    "    # not found\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# ---------- Vectorized helpers ----------\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2:\n",
    "            return \" \".join(parts[1:] + parts[:1])  # move leading token to end\n",
    "        return name\n",
    "\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return pd.Series(num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\"), index=s.index, dtype=\"string\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# ---------- Main compiler/executor ----------\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply LLM-compiled rule_spec to df and return a DataFrame with FINAL_COLUMNS.\n",
    "    Supported spec entry for each target:\n",
    "      {\"op\":\"copy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"const\",\"value\":\"Active\"}\n",
    "      {\"op\":\"date_mmddyyyy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"date_plus_1m_mmddyyyy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"name_first_from_full\",\"source\":\"Col\"}\n",
    "      {\"op\":\"name_last_from_full\",\"source\":\"Col\"}\n",
    "      {\"op\":\"money\",\"source\":\"Col\"}\n",
    "      {\"op\":\"membercount_from_commission\",\"source\":\"Col\"}  # \"1\" unless <0 → \"-1\"; blank→\"1\"\n",
    "      {\"op\":\"blank\"}\n",
    "    Special: if spec contains \"PID\", it is written to output \"PTD\".\n",
    "    \"\"\"\n",
    "    header_index = _build_header_index(df)\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    out: dict[str, pd.Series] = {}\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        # Allow alias: PID (rule) -> PTD (output)\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not spec or not isinstance(spec, dict):\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        op = str(spec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            out[tgt] = _get_col(df, header_index, spec.get(\"source\", \"\"))\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series([str(spec.get(\"value\", \"\"))] * len(df), index=df.index, dtype=\"string\")\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            first, _ = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = first\n",
    "        elif op == \"name_last_from_full\":\n",
    "            _, last = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = last\n",
    "        elif op == \"money\":\n",
    "            out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif op == \"blank\":\n",
    "            out[tgt] = empty()\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    # Ensure order + types\n",
    "    out_df = pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "    return out_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
