{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ca6086",
   "metadata": {},
   "source": [
    "\n",
    "# Manhattan Life LLM Processor Test Notebook\n",
    "\n",
    "This notebook contains **all code inline** (no external imports of your local modules)\n",
    "so you can test the Manhattan Life mapping including **PlanCode** end‑to‑end.\n",
    "\n",
    "1. Update the configuration values below (CSV path, template dir, DB info).\n",
    "2. Run all cells from top to bottom.\n",
    "3. The pipeline will produce an output CSV next to your input file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "\n",
    "issuer = \"Manhattan Life\"\n",
    "paycode = \"Default\"\n",
    "trandate = \"2025-11-12\"   # or any date in mm/dd/yyyy or yyyy-mm-dd\n",
    "load_task_id = \"11497\"\n",
    "company_issuer_id = \"3205\"\n",
    "\n",
    "# Path to your raw Manhattan Life CSV\n",
    "csv_path = \"./inbound/raw - 9836c995-9e25-45ae-af39-b44cc8ac1bbd.csv\"\n",
    "\n",
    "# Folder that contains `Manhattan Life_prompt.txt` and `Manhattan Life_rules.json`\n",
    "template_dir = \"./carrier_prompts\"\n",
    "\n",
    "# SQL Server connection info\n",
    "server_name = \"NGCS\"        # e.g. \"YOURSQLSERVER\"\n",
    "database_name = \"NGCS\"      # e.g. \"YourDatabase\"\n",
    "\n",
    "# Name (without extension) for the output CSV\n",
    "output_csv_name = \"manhattan_life_out_test\"\n",
    "\n",
    "# Azure OpenAI config (must be set in your environment)\n",
    "# AZURE_OPENAI_ENDPOINT\n",
    "# AZURE_OPENAI_API_KEY\n",
    "# AZURE_OPENAI_DEPLOYMENT_NAME\n",
    "# AZURE_OPENAI_API_VERSION\n",
    "\n",
    "print(\"Config loaded. Update values above as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Optional: LangChain Azure OpenAI\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:  # legacy fallback\n",
    "    try:\n",
    "        from langchain.chat_models import AzureChatOpenAI  # type: ignore\n",
    "    except Exception:\n",
    "        AzureChatOpenAI = None  # type: ignore\n",
    "\n",
    "# Optional: Ray for parallelism (not required here, kept for compatibility)\n",
    "try:\n",
    "    import ray\n",
    "except Exception:  # pragma: no cover\n",
    "    ray = None  # type: ignore\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Perf / output config (env)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "ENABLE_RAY = os.getenv(\"ENABLE_RAY\", \"auto\")  # \"auto\" | \"on\" | \"off\"\n",
    "RAY_PARTITIONS = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve().parent\n",
    "OUT_DIR = Path(os.getenv(\"OUT_DIR\", BASE_DIR / \"outbound\")).resolve()\n",
    "OUT_FORMAT = os.getenv(\"OUT_FORMAT\", \"csv\")  # \"parquet\" | \"csv\"\n",
    "PARQUET_COMPRESSION = os.getenv(\"PARQUET_COMPRESSION\", \"snappy\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./carrier_prompts\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./uploads\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Carrier config & schema\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "CARRIERS: Dict[str, Dict[str, Any]] = {\n",
    "    \"Molina\": {\"loader\": \"csv\"},\n",
    "    \"Ameritas\": {\"loader\": \"csv\"},\n",
    "    \"Manhattan Life\": {\"loader\": \"two_header\"},  # 2-row header CSV\n",
    "}\n",
    "\n",
    "FINAL_COLUMNS: List[str] = [\n",
    "    \"PolicyNO\",\n",
    "    \"PHFirst\",\n",
    "    \"PHLast\",\n",
    "    \"Status\",\n",
    "    \"Issuer\",\n",
    "    \"State\",\n",
    "    \"ProductType\",\n",
    "    \"PlanName\",\n",
    "    \"PlanCode\",        # <-- PlanCode included\n",
    "    \"SubmittedDate\",\n",
    "    \"EffectiveDate\",\n",
    "    \"TermDate\",\n",
    "    \"Paysched\",\n",
    "    \"PayCode\",\n",
    "    \"WritingAgentID\",\n",
    "    \"Premium\",\n",
    "    \"CommPrem\",\n",
    "    \"TranDate\",\n",
    "    \"CommReceived\",\n",
    "    \"PTD\",\n",
    "    \"NoPayMon\",\n",
    "    \"Membercount\",\n",
    "]\n",
    "\n",
    "ALLOWED_OPS: List[str] = [\n",
    "    \"copy\",\n",
    "    \"const\",\n",
    "    \"date_mmddyyyy\",\n",
    "    \"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\n",
    "    \"name_last_from_full\",\n",
    "    \"money\",\n",
    "    \"membercount_from_commission\",\n",
    "    \"blank\",\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Return STRICT JSON ONLY (no prose). The top-level JSON object must contain EXACTLY the required keys.\n",
    "For each key return an object with:\n",
    "- \"op\": one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,\n",
    "                 money,membercount_from_commission,blank]\n",
    "- \"source\": the exact input column name when applicable (for ops that read input)\n",
    "- \"value\": for const\n",
    "If unclear, use {\"op\":\"blank\"}.\n",
    "You MAY also include \"PID\" as a key if your rules produce it; downstream will map PID -> PTD.\n",
    "Do not add extra keys. Do not omit required keys.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _fast_read_header(path: str, loader: str) -> List[str]:\n",
    "    \"\"\"Fast probe: return list of column names without reading full file.\"\"\"\n",
    "    p = str(path)\n",
    "    if loader == \"csv\":\n",
    "        try:\n",
    "            dfo = pd.read_csv(p, nrows=0, dtype=str, engine=\"pyarrow\", memory_map=True)\n",
    "        except Exception:\n",
    "            dfo = pd.read_csv(p, nrows=0, dtype=str, low_memory=False)\n",
    "        return list(dfo.columns)\n",
    "\n",
    "    # two_header: first two rows -> synthetic headers\n",
    "    probe = pd.read_csv(p, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = probe.iloc[0].tolist(), probe.iloc[1].tolist()\n",
    "\n",
    "    ff: List[str] = []\n",
    "    last = \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "        cols.append(name or \"unnamed\")\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def _sig_from_cols(cols: List[str]) -> str:\n",
    "    norm = [re.sub(r\"\\s+\", \" \", c.strip().lower()) for c in cols]\n",
    "    raw = \"|\".join(norm).encode(\"utf-8\")\n",
    "    return hashlib.md5(raw).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def _ensure_out_dir() -> None:\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _create_llm() -> AzureChatOpenAI:\n",
    "    if AzureChatOpenAI is None:\n",
    "        raise RuntimeError(\"AzureChatOpenAI is not available in this environment.\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "    if not (deployment and endpoint and api_key):\n",
    "        raise RuntimeError(\"Missing Azure OpenAI environment variables.\")\n",
    "\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "        api_version=api_version,\n",
    "        temperature=0.0,\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_generate_rule_spec(\n",
    "    headers: List[str],\n",
    "    prompt_path: Path,\n",
    "    rules_path: Path,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Call Azure OpenAI to generate a mapping spec from headers.\"\"\"\n",
    "    extra_prompt = \"\"\n",
    "    if prompt_path.exists():\n",
    "        extra_prompt = prompt_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    explicit_rules: Dict[str, Any] = {}\n",
    "    if rules_path.exists():\n",
    "        explicit_rules = json.loads(rules_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    required_fields = list(FINAL_COLUMNS) + [\"PID\"]\n",
    "    payload = {\n",
    "        \"Headers\": headers,\n",
    "        \"RequiredFields\": required_fields,\n",
    "        \"AllowedOps\": ALLOWED_OPS,\n",
    "        \"ExplicitFieldHints\": explicit_rules,\n",
    "        \"ExtraPrompt\": extra_prompt,\n",
    "        \"OutputFormat\": \"Return a JSON object keyed by RequiredFields (plus PID if used).\",\n",
    "    }\n",
    "\n",
    "    llm = _create_llm()\n",
    "    resp = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = getattr(resp, \"content\", str(resp))\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        raise ValueError(f\"LLM did not return valid JSON:\\n{content}\") from e\n",
    "\n",
    "\n",
    "def canonicalize_spec(spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Ensure spec has a dict per field with 'op', and optional 'source' / 'value'.\"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "    for field, rule in spec.items():\n",
    "        if isinstance(rule, str):\n",
    "            out[field] = {\"op\": rule}\n",
    "        elif isinstance(rule, dict):\n",
    "            op = rule.get(\"op\")\n",
    "            if op not in ALLOWED_OPS:\n",
    "                op = \"blank\"\n",
    "            obj = {\"op\": op}\n",
    "            if \"source\" in rule:\n",
    "                obj[\"source\"] = rule[\"source\"]\n",
    "            if \"value\" in rule:\n",
    "                obj[\"value\"] = rule[\"value\"]\n",
    "            out[field] = obj\n",
    "        else:\n",
    "            out[field] = {\"op\": \"blank\"}\n",
    "    return out\n",
    "\n",
    "\n",
    "def bind_sources_to_headers(headers: List[str], spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Try to match the 'source' in each rule to an actual header name ignoring minor spacing.\"\"\"\n",
    "    normalized_header_map: Dict[str, str] = {}\n",
    "    for h in headers:\n",
    "        norm = re.sub(r\"[^a-z0-9]\", \"\", h.lower())\n",
    "        normalized_header_map[norm] = h\n",
    "\n",
    "    bound: Dict[str, Any] = {}\n",
    "    for field, rule in spec.items():\n",
    "        rule = dict(rule)\n",
    "        src = rule.get(\"source\")\n",
    "        if isinstance(src, str):\n",
    "            norm_src = re.sub(r\"[^a-z0-9]\", \"\", src.lower())\n",
    "            if norm_src in normalized_header_map:\n",
    "                rule[\"source\"] = normalized_header_map[norm_src]\n",
    "        bound[field] = rule\n",
    "    return bound\n",
    "\n",
    "\n",
    "def promote_pid_to_ptd(bound_spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"If the spec includes a PID rule, copy it as PTD (Paid to Date).\"\"\"\n",
    "    out = dict(bound_spec)\n",
    "    if \"PID\" in out:\n",
    "        out[\"PTD\"] = dict(out[\"PID\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Rule application\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _apply_op(op: str, v: Any, row: pd.Series, rule: Dict[str, Any]) -> Any:\n",
    "    if op == \"blank\":\n",
    "        return \"\"\n",
    "    if op == \"const\":\n",
    "        return rule.get(\"value\", \"\")\n",
    "    if op == \"copy\":\n",
    "        return v or \"\"\n",
    "    if op == \"date_mmddyyyy\":\n",
    "        if not v:\n",
    "            return \"\"\n",
    "        s = str(v).strip()\n",
    "        m = re.match(r\"^(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})$\", s)\n",
    "        if m:\n",
    "            mm, dd, yy = m.groups()\n",
    "            if len(yy) == 2:\n",
    "                yy = \"20\" + yy\n",
    "            return f\"{int(mm):02d}/{int(dd):02d}/{yy}\"\n",
    "        m2 = re.match(r\"^(\\d{4})-(\\d{1,2})-(\\d{1,2})$\", s)\n",
    "        if m2:\n",
    "            yy, mm, dd = map(int, m2.groups())\n",
    "            return f\"{mm:02d}/{dd:02d}/{yy}\"\n",
    "        return s\n",
    "    if op == \"date_plus_1m_mmddyyyy\":\n",
    "        base = _apply_op(\"date_mmddyyyy\", v, row, rule)\n",
    "        if not base:\n",
    "            return \"\"\n",
    "        try:\n",
    "            mm, dd, yy = map(int, base.split(\"/\"))\n",
    "            dt = pd.Timestamp(year=yy, month=mm, day=dd)\n",
    "            dt2 = dt + relativedelta(months=1)\n",
    "            return f\"{dt2.month:02d}/{dt2.day:02d}/{dt2.year}\"\n",
    "        except Exception:\n",
    "            return base\n",
    "    if op == \"name_first_from_full\":\n",
    "        s = str(v or \"\").strip()\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        parts = s.split()\n",
    "        if len(parts) <= 1:\n",
    "            return parts[0]\n",
    "        return \" \".join(parts[:-1])\n",
    "    if op == \"name_last_from_full\":\n",
    "        s = str(v or \"\").strip()\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        parts = s.split()\n",
    "        return parts[-1]\n",
    "    if op == \"money\":\n",
    "        if v is None or (isinstance(v, float) and math.isnan(v)):\n",
    "            return \"\"\n",
    "        try:\n",
    "            return float(str(v).replace(\",\", \"\").strip())\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    if op == \"membercount_from_commission\":\n",
    "        amt = _apply_op(\"money\", v, row, rule)\n",
    "        if amt == \"\":\n",
    "            return 1\n",
    "        try:\n",
    "            return -1 if float(amt) < 0 else 1\n",
    "        except Exception:\n",
    "            return 1\n",
    "    return v or \"\"\n",
    "\n",
    "\n",
    "def apply_rules(df: pd.DataFrame, spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=FINAL_COLUMNS)\n",
    "\n",
    "    spec = canonicalize_spec(spec)\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        df2[c] = df2[c].astype(str)\n",
    "\n",
    "    out_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, row in df2.iterrows():\n",
    "        rec: Dict[str, Any] = {}\n",
    "        for field in FINAL_COLUMNS + [\"PID\"]:\n",
    "            rule = spec.get(field, {\"op\": \"blank\"})\n",
    "            op = rule.get(\"op\", \"blank\")\n",
    "            src = rule.get(\"source\")\n",
    "            val = None\n",
    "            if src and src in row:\n",
    "                val = row[src]\n",
    "            rec[field] = _apply_op(op, val, row, rule)\n",
    "        if \"PID\" in rec and \"PTD\" in FINAL_COLUMNS:\n",
    "            rec[\"PTD\"] = rec.get(\"PID\", \"\")\n",
    "        out_records.append(rec)\n",
    "\n",
    "    out_df = pd.DataFrame(out_records)\n",
    "    if \"PolicyNO\" in out_df.columns and \"PolicyNo\" not in out_df.columns:\n",
    "        out_df[\"PolicyNo\"] = out_df[\"PolicyNO\"]\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Manhattan Life helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def extract_manhattan_policy_plan_from_csv(csv_path: str, log: Callable[[str], None]) -> pd.DataFrame:\n",
    "    p = str(csv_path).strip().strip('\"').strip(\"'\")\n",
    "    log(f\"[ManhattanLife] Reading raw CSV: {p}\")\n",
    "\n",
    "    read_kwargs = dict(dtype=str, engine=\"python\")\n",
    "    try:\n",
    "        raw = pd.read_csv(p, header=[0, 1], encoding=\"utf-8-sig\", **read_kwargs)\n",
    "    except Exception:\n",
    "        try:\n",
    "            raw = pd.read_csv(p, header=0, encoding=\"utf-8-sig\", **read_kwargs)\n",
    "        except Exception:\n",
    "            raw = pd.read_csv(p, header=0, encoding=\"latin1\", **read_kwargs)\n",
    "\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        flat: List[str] = []\n",
    "        for parts in raw.columns:\n",
    "            parts = [str(x).strip() for x in parts if x is not None and str(x).strip() != \"\"]\n",
    "            name = re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "            flat.append(name)\n",
    "        raw.columns = flat\n",
    "    else:\n",
    "        raw.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in raw.columns]\n",
    "\n",
    "    norm = {c: re.sub(r\"[^A-Za-z0-9]\", \"\", c.lower()) for c in raw.columns}\n",
    "\n",
    "    plan_code_col: Optional[str] = None\n",
    "    for col, nc in norm.items():\n",
    "        if \"plan\" in nc and \"code\" in nc:\n",
    "            plan_code_col = col\n",
    "            break\n",
    "    if plan_code_col is None:\n",
    "        for col, nc in norm.items():\n",
    "            if nc.startswith(\"plancode\"):\n",
    "                plan_code_col = col\n",
    "                break\n",
    "\n",
    "    policy_col: Optional[str] = None\n",
    "    for col, nc in norm.items():\n",
    "        if \"policynumber\" in nc or \"policyno\" in nc or \"casenumber\" in nc:\n",
    "            policy_col = col\n",
    "            break\n",
    "\n",
    "    if policy_col is None:\n",
    "        candidates = [c for c in raw.columns if \"policy\" in c.lower() or \"case\" in c.lower()]\n",
    "        if candidates:\n",
    "            def numeric_ratio(series: pd.Series) -> float:\n",
    "                s = series.dropna().astype(str).str.strip()\n",
    "                if len(s) == 0:\n",
    "                    return 0.0\n",
    "                m = s.str.match(r\"^\\d{5,}$\")\n",
    "                return float(m.mean())\n",
    "\n",
    "            policy_col = max(candidates, key=lambda c: numeric_ratio(raw[c]))\n",
    "\n",
    "    if policy_col is None or plan_code_col is None:\n",
    "        raise ValueError(\n",
    "            \"Could not locate Policy/PlanCode columns. \"\n",
    "            f\"Seen headers: {list(raw.columns)[:20]}\"\n",
    "        )\n",
    "\n",
    "    df2 = raw[[policy_col, plan_code_col]].copy()\n",
    "    df2.columns = [\"PolicyNumber\", \"PlanCode\"]\n",
    "    df2[\"PolicyNumber\"] = df2[\"PolicyNumber\"].astype(str).str.strip()\n",
    "    df2[\"PlanCode\"] = df2[\"PlanCode\"].astype(str).str.strip().str.upper()\n",
    "    df2 = df2[df2[\"PolicyNumber\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    log(\n",
    "        f\"[ManhattanLife] Extracted rows: {len(df2)} | \"\n",
    "        f\"cols -> PolicyNumber: {policy_col}, PlanCode: {plan_code_col}\"\n",
    "    )\n",
    "    return df2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DB helpers (simple stubs by default)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _get_sql_connection(server: str, database: str):\n",
    "    \"\"\"Return a pyodbc connection if available. Edit to your environment.\"\"\"\n",
    "    import pyodbc  # type: ignore\n",
    "\n",
    "    driver = os.getenv(\"SQL_DRIVER\", \"{ODBC Driver 17 for SQL Server}\")\n",
    "    user = os.getenv(\"SQL_USER\")\n",
    "    password = os.getenv(\"SQL_PASSWORD\")\n",
    "\n",
    "    if user and password:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={driver};SERVER={server};DATABASE={database};\"\n",
    "            f\"UID={user};PWD={password}\"\n",
    "        )\n",
    "    else:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={driver};SERVER={server};DATABASE={database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "\n",
    "    return pyodbc.connect(conn_str)\n",
    "\n",
    "\n",
    "def insert_stg_plan_mapping_min(\n",
    "    df: pd.DataFrame,\n",
    "    load_task_id: str,\n",
    "    company_issuer_id: str,\n",
    "    server: str,\n",
    "    database: str,\n",
    "    log: Callable[[str], None],\n",
    ") -> int:\n",
    "    if df.empty:\n",
    "        return 0\n",
    "\n",
    "    conn = _get_sql_connection(server, database)\n",
    "    cur = conn.cursor()\n",
    "    inserted = 0\n",
    "    try:\n",
    "        for _, row in df.iterrows():\n",
    "            policy = row[\"PolicyNumber\"]\n",
    "            plan_id = row[\"PlanId\"]\n",
    "            cur.execute(\n",
    "                \"EXEC dbo.spInsertStgPlanMappingMin ?, ?, ?, ?\",\n",
    "                load_task_id,\n",
    "                company_issuer_id,\n",
    "                policy,\n",
    "                plan_id,\n",
    "            )\n",
    "            inserted += 1\n",
    "        conn.commit()\n",
    "        log(f\"[DB] insert_stg_plan_mapping_min inserted rows: {inserted}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    return inserted\n",
    "\n",
    "\n",
    "def get_manhattan_mapping(\n",
    "    Load_task_id: str,\n",
    "    company_issuer_id: str,\n",
    "    server: str,\n",
    "    database: str,\n",
    "    log: Callable[[str], None],\n",
    ") -> pd.DataFrame:\n",
    "    conn = _get_sql_connection(server, database)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\n",
    "            \"EXEC dbo.spGetManhattanMapping ?, ?\",\n",
    "            Load_task_id,\n",
    "            company_issuer_id,\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "        cols = [c[0] for c in cur.description]\n",
    "        df = pd.DataFrame.from_records(rows, columns=cols)\n",
    "        log(f\"[DB] get_manhattan_mapping fetched rows: {len(df)}\")\n",
    "        return df\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: match_llm_output_to_raw_counts\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def match_llm_output_to_raw_counts(raw_link_df: pd.DataFrame, out_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if out_df.empty or raw_link_df.empty:\n",
    "        return out_df\n",
    "\n",
    "    counts = (\n",
    "        raw_link_df[\"PolicyNumber\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .value_counts()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    out_df = out_df.copy()\n",
    "    out_df[\"PolicyNo\"] = out_df[\"PolicyNo\"].astype(str).str.strip()\n",
    "\n",
    "    rows: List[pd.Series] = []\n",
    "    for _, row in out_df.iterrows():\n",
    "        pol = row[\"PolicyNo\"]\n",
    "        n = counts.get(pol, 1)\n",
    "        for _ in range(int(n)):\n",
    "            rows.append(row.copy())\n",
    "\n",
    "    return pd.DataFrame(rows, columns=out_df.columns)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def run_llm_pipeline(\n",
    "    *,\n",
    "    issuer: str,\n",
    "    paycode: str,\n",
    "    trandate: str,\n",
    "    load_task_id: str,\n",
    "    company_issuer_id: str,\n",
    "    csv_path: str,\n",
    "    template_dir: str,\n",
    "    output_csv_name: str,\n",
    "    server_name: str,\n",
    "    database_name: str,\n",
    "    log: Callable[[str], None] = print,\n",
    ") -> str:\n",
    "    start = time.perf_counter()\n",
    "    log(f\"Starting LLM pipeline | issuer={issuer} | csv={csv_path}\")\n",
    "\n",
    "    loader = CARRIERS.get(issuer, {}).get(\"loader\", \"csv\")\n",
    "    prompt_path = Path(template_dir) / f\"{issuer}_prompt.txt\"\n",
    "    rules_path = Path(template_dir) / f\"{issuer}_rules.json\"\n",
    "\n",
    "    if not prompt_path.exists():\n",
    "        log(f\"NOTE: prompt file not found, continuing: {prompt_path}\")\n",
    "    if not rules_path.exists():\n",
    "        log(f\"NOTE: rules file not found, continuing: {rules_path}\")\n",
    "\n",
    "    headers = _fast_read_header(csv_path, loader)\n",
    "    sig = _sig_from_cols(headers)\n",
    "    compiled_path = Path(template_dir) / f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "    if compiled_path.exists():\n",
    "        log(f\"[Rules] Loaded cached compiled rules: {compiled_path.name}\")\n",
    "        bound_spec = json.loads(compiled_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        log(\"[Rules] Generating with LLM.\")\n",
    "        raw_spec = llm_generate_rule_spec(headers, prompt_path, rules_path)\n",
    "        raw_spec = canonicalize_spec(raw_spec)\n",
    "        bound_spec = bind_sources_to_headers(headers, raw_spec)\n",
    "        bound_spec = promote_pid_to_ptd(bound_spec)\n",
    "        compiled_path.write_text(\n",
    "            json.dumps(bound_spec, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        log(f\"[Rules] Compiled rules written to: {compiled_path.name}\")\n",
    "\n",
    "    if loader == \"csv\":\n",
    "        df = pd.read_csv(csv_path, dtype=str)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "    log(f\"[Input] rows={len(df)}, cols={list(df.columns)}\")\n",
    "    out_df = apply_rules(df, bound_spec)\n",
    "\n",
    "    out_df[\"TranDate\"] = trandate\n",
    "    out_df[\"PayCode\"] = paycode\n",
    "    out_df[\"Issuer\"] = issuer\n",
    "    if \"ProductType\" not in out_df.columns:\n",
    "        out_df[\"ProductType\"] = \"\"\n",
    "    if \"PlanName\" not in out_df.columns:\n",
    "        out_df[\"PlanName\"] = \"\"\n",
    "    if \"PlanCode\" not in out_df.columns:\n",
    "        out_df[\"PlanCode\"] = \"\"\n",
    "\n",
    "    if issuer.lower().replace(\" \", \"\") in {\"manhattanlife\", \"manhattenlife\"}:\n",
    "        log(\"[ManhattanLife] Starting PlanCode / mapping enrichment step.\")\n",
    "\n",
    "        raw_link_df = extract_manhattan_policy_plan_from_csv(csv_path, log)\n",
    "        r = raw_link_df.copy()\n",
    "        r[\"PolicyNo\"] = r[\"PolicyNumber\"]\n",
    "\n",
    "        payload = raw_link_df[[\"PolicyNumber\", \"PlanCode\"]].copy()\n",
    "        payload.columns = [\"PolicyNumber\", \"PlanId\"]\n",
    "\n",
    "        try:\n",
    "            inserted = insert_stg_plan_mapping_min(\n",
    "                payload,\n",
    "                load_task_id=load_task_id,\n",
    "                company_issuer_id=company_issuer_id,\n",
    "                server=server_name,\n",
    "                database=database_name,\n",
    "                log=log,\n",
    "            )\n",
    "            log(f\"[ManhattanLife] STG insert-min rows: {inserted}\")\n",
    "        except Exception as e:\n",
    "            log(f\"[ManhattanLife][WARN] insert_stg_plan_mapping_min failed: {e}\")\n",
    "            inserted = 0\n",
    "\n",
    "        try:\n",
    "            map_df = get_manhattan_mapping(\n",
    "                Load_task_id=load_task_id,\n",
    "                company_issuer_id=company_issuer_id,\n",
    "                server=server_name,\n",
    "                database=database_name,\n",
    "                log=log,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log(f\"[ManhattanLife][WARN] get_manhattan_mapping failed: {e}\")\n",
    "            map_df = pd.DataFrame()\n",
    "\n",
    "        out_df_cols = [\n",
    "            \"PolicyNo\",\n",
    "            \"PHFirst\",\n",
    "            \"PHLast\",\n",
    "            \"Status\",\n",
    "            \"Issuer\",\n",
    "            \"State\",\n",
    "            \"ProductType\",\n",
    "            \"PlanName\",\n",
    "            \"PlanCode\",\n",
    "            \"SubmittedDate\",\n",
    "            \"EffectiveDate\",\n",
    "            \"TermDate\",\n",
    "            \"Paysched\",\n",
    "            \"PayCode\",\n",
    "            \"WritingAgentID\",\n",
    "            \"Premium\",\n",
    "            \"CommPrem\",\n",
    "            \"TranDate\",\n",
    "            \"CommReceived\",\n",
    "            \"PTD\",\n",
    "            \"NoPayMon\",\n",
    "            \"Membercount\",\n",
    "            \"Note\",\n",
    "        ]\n",
    "\n",
    "        if \"PolicyNo\" not in out_df.columns and \"PolicyNO\" in out_df.columns:\n",
    "            out_df[\"PolicyNo\"] = out_df[\"PolicyNO\"]\n",
    "\n",
    "        out_df2 = out_df.copy()\n",
    "\n",
    "        if map_df.empty:\n",
    "            log(\"[ManhattanLife] No mapping returned from DB; using raw PlanCode only.\")\n",
    "            payload_df = raw_link_df[[\"PolicyNumber\", \"PlanCode\"]].copy()\n",
    "            payload_df.columns = [\"PolicyNo\", \"PlanCode\"]\n",
    "            out_df2 = out_df2.merge(payload_df, on=\"PolicyNo\", how=\"left\")\n",
    "            out_df2[\"PlanName\"] = out_df2.get(\"PlanName\", \"\")\n",
    "            out_df2[\"ProductType\"] = out_df2.get(\"ProductType\", \"\")\n",
    "            out_df2 = out_df2[out_df_cols]\n",
    "            out_df2 = out_df2.fillna(\"\")\n",
    "            out_df = out_df2.copy()\n",
    "            out_df = match_llm_output_to_raw_counts(r, out_df)\n",
    "        else:\n",
    "            cols_lower = {c.lower(): c for c in map_df.columns}\n",
    "            need_cols = {}\n",
    "            for key in [\"policyno\", \"planname\", \"producttype\", \"plancode\"]:\n",
    "                if key in cols_lower:\n",
    "                    need_cols[key] = cols_lower[key]\n",
    "\n",
    "            if \"plancode\" in need_cols:\n",
    "                map_df2 = map_df[\n",
    "                    [\n",
    "                        need_cols[\"policyno\"],\n",
    "                        need_cols[\"planname\"],\n",
    "                        need_cols[\"producttype\"],\n",
    "                        need_cols[\"plancode\"],\n",
    "                    ]\n",
    "                ]\n",
    "                map_df2.columns = [\"PolicyNo\", \"PlanName\", \"ProductType\", \"PlanCode\"]\n",
    "            else:\n",
    "                map_df2 = map_df[\n",
    "                    [\n",
    "                        need_cols[\"policyno\"],\n",
    "                        need_cols[\"planname\"],\n",
    "                        need_cols[\"producttype\"],\n",
    "                    ]\n",
    "                ]\n",
    "                map_df2.columns = [\"PolicyNo\", \"PlanName\", \"ProductType\"]\n",
    "\n",
    "            out_df2 = out_df2.merge(map_df2, on=\"PolicyNo\", how=\"left\")\n",
    "\n",
    "            payload_df = raw_link_df[[\"PolicyNumber\", \"PlanCode\"]].copy()\n",
    "            payload_df.columns = [\"PolicyNo\", \"PlanCode_raw\"]\n",
    "            out_df2 = out_df2.merge(payload_df, on=\"PolicyNo\", how=\"left\")\n",
    "\n",
    "            if \"PlanCode\" in out_df2.columns:\n",
    "                out_df2[\"PlanCode\"] = out_df2[\"PlanCode\"].fillna(out_df2[\"PlanCode_raw\"])\n",
    "            else:\n",
    "                out_df2[\"PlanCode\"] = out_df2[\"PlanCode_raw\"]\n",
    "\n",
    "            out_df2.drop(columns=[\"PlanCode_raw\"], inplace=True)\n",
    "\n",
    "            out_df2 = out_df2[out_df_cols]\n",
    "            out_df2 = out_df2.fillna(\"\")\n",
    "            out_df = out_df2.copy()\n",
    "            out_df = match_llm_output_to_raw_counts(r, out_df)\n",
    "\n",
    "    _ensure_out_dir()\n",
    "\n",
    "    input_path = Path(csv_path)\n",
    "    out_dir = input_path.parent\n",
    "    if output_csv_name:\n",
    "        out_path = out_dir / f\"{output_csv_name}.csv\"\n",
    "    else:\n",
    "        out_path = out_dir / (input_path.stem + \"_out.csv\")\n",
    "\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    log(f\"[Output] file written sucessfully: {out_path}\")\n",
    "    log(f\"Completed: {out_path.as_posix()} (elapsed {elapsed:.2f}s)\")\n",
    "    return out_path.as_posix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c47ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Run the pipeline with config\n",
    "# ------------------------------\n",
    "\n",
    "output_path = run_llm_pipeline(\n",
    "    issuer=issuer,\n",
    "    paycode=paycode,\n",
    "    trandate=trandate,\n",
    "    load_task_id=load_task_id,\n",
    "    company_issuer_id=company_issuer_id,\n",
    "    csv_path=csv_path,\n",
    "    template_dir=template_dir,\n",
    "    output_csv_name=output_csv_name,\n",
    "    server_name=server_name,\n",
    "    database_name=database_name,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal output path:\", output_path)\n",
    "\n",
    "# Show a sample of the output (if you are in Jupyter)\n",
    "try:\n",
    "    display(pd.read_csv(output_path).head())\n",
    "except Exception as e:\n",
    "    print(\"Could not preview output:\", e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
