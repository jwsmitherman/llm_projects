{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.py\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, re, json, hashlib, time, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# LangChain Azure OpenAI\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Perf / output config (env)\n",
    "ENABLE_RAY          = os.getenv(\"ENABLE_RAY\", \"auto\")      # \"auto\" | \"on\" | \"off\"\n",
    "RAY_PARTITIONS      = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "BASE_DIR  = Path(__file__).resolve().parent\n",
    "OUT_DIR   = Path(os.getenv(\"OUT_DIR\", BASE_DIR / \"outbound\")).resolve()\n",
    "OUT_FORMAT = os.getenv(\"OUT_FORMAT\", \"csv\")               # \"parquet\" | \"csv\"\n",
    "PARQUET_COMPRESSION = os.getenv(\"PARQUET_COMPRESSION\", \"snappy\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./carrier-prompts\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./uploads\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Per-carrier Loader modes\n",
    "CARRIERS = {\n",
    "    \"Molina\": {\n",
    "        \"loader\": \"csv\",             # one-row header\n",
    "    },\n",
    "    \"Ameritas\": {\n",
    "        \"loader\": \"csv\"\n",
    "    },\n",
    "    \"Manhattan Life\": {\n",
    "        \"loader\": \"two_header\"       # two-row header (flatten)\n",
    "    },\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# schema / ops\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# ðŸ”¹ CHANGE 1: add PlanCode into FINAL_COLUMNS (between PlanName and SubmittedDate)\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNO\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\"PlanCode\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"Paysched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = [\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "    \"membercount_from_commission\",\"blank\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Return STRICT JSON ONLY (no prose). The top-level JSON object must contain EXACTLY the required keys.\n",
    "For each key return an object with:\n",
    "- \"op\": one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,\n",
    "                 money,membercount_from_commission,blank]\n",
    "- \"source\": the exact input column name when applicable (for ops that read input)\n",
    "- \"value\": for const\n",
    "If unclear, use {\"op\":\"blank\"}.\n",
    "You MAY also include \"PID\" as a key if your rules produce it; downstream will map PID -> PID.\n",
    "Do not add extra keys. Do not omit required keys.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Small utils\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _norm_key(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "\n",
    "def _sig_from_cols(cols: List[str]) -> str:\n",
    "    joined = \"||\".join(map(str, cols))\n",
    "    return hashlib.sha1(joined.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def _build_header_index(cols: List[str]) -> Dict[str, str]:\n",
    "    return {_norm_key(h): h for h in cols}\n",
    "\n",
    "def _load_text(p: Path) -> str:\n",
    "    return p.read_text(encoding=\"utf-8\") if p.exists() else \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loader-aware header probe and readers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _fast_read_header(path: str, loader: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fast probe: return list of column names without reading full file.\n",
    "    Supports \"csv\" and \"two_header\".\n",
    "    \"\"\"\n",
    "    if loader == \"csv\":\n",
    "        try:\n",
    "            dfo = pd.read_csv(path, nrows=0, dtype=str, engine=\"pyarrow\", memory_map=True)\n",
    "        except Exception:\n",
    "            dfo = pd.read_csv(path, nrows=0, dtype=str, low_memory=False)\n",
    "        return list(dfo.columns)\n",
    "\n",
    "    # two_header: first two rows -> synthetic headers\n",
    "    probe = pd.read_csv(path, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = probe.iloc[0].tolist(), probe.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def _read_csv_usecols(path: str,\n",
    "                      usecols: Optional[List[str]],\n",
    "                      loader: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV with optional usecols and support for two_header loader.\n",
    "    \"\"\"\n",
    "    if loader == \"csv\":\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                path, dtype=str, engine=\"pyarrow\", memory_map=True,\n",
    "                usecols=usecols if usecols else None\n",
    "            ).fillna(\"\")\n",
    "        except Exception:\n",
    "            return pd.read_csv(\n",
    "                path, dtype=str, low_memory=False,\n",
    "                usecols=usecols if usecols else None\n",
    "            ).fillna(\"\")\n",
    "\n",
    "    # two_header full read then filter\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    keep = [c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]\n",
    "    df = df[keep]\n",
    "\n",
    "    if usecols:\n",
    "        present = [c for c in usecols if c in df.columns]\n",
    "        return df[present].copy()\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_llm(timeout: int = 30, temperature: float = 0.0) -> AzureChatOpenAI:\n",
    "    # NOTE: for safety I removed your real key here. Put your key back locally.\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_endpoint   = \"https://joshu-meub0vlp-swedencentral.cognitiveservices.azure.com/\",\n",
    "        api_key          = \"REPLACE_WITH_YOUR_REAL_KEY\",\n",
    "        api_version      = \"2024-12-01-preview\",\n",
    "        azure_deployment = \"gpt-5-mini_ng\",\n",
    "        temperature      = temperature,\n",
    "        timeout          = timeout,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def llm_generate_rule_spec(headers: List[str],\n",
    "                           prompt_path: Path,\n",
    "                           rules_path: Path) -> Dict[str, Any]:\n",
    "    llm = build_llm()\n",
    "    payload = {\n",
    "        # ðŸ”¹ Because FINAL_COLUMNS now includes PlanCode,\n",
    "        #     the LLM knows it must return a rule for PlanCode as well.\n",
    "        \"RequiredFields\": FINAL_COLUMNS + [\"PID\"],   # allow PID alias\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": _load_text(rules_path),    # narrative JSON or text\n",
    "        \"ExtraPrompt\": _load_text(prompt_path),      # extra instructions\n",
    "        \"OutputFormat\": \"Return a JSON object keyed by RequiredFields (plus PID if used).\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "    ])\n",
    "    content = getattr(resp, \"content\", str(resp))\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"LLM did not return valid JSON:\\n{content}\") from e\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spec normalization / binding\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "_CANON = {_norm_key(k): k for k in FINAL_COLUMNS + [\"PID\"]}\n",
    "\n",
    "def canonicalize_spec(spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        nk = _norm_key(k)\n",
    "        fixed[_CANON.get(nk, k)] = v\n",
    "    for req in FINAL_COLUMNS:\n",
    "        if req not in fixed and (req != \"PTD\" or \"PID\" not in fixed):\n",
    "            fixed[req] = {\"op\": \"blank\"}\n",
    "    return fixed\n",
    "\n",
    "\n",
    "def normalize_rule_spec(spec_in: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            sv = v.strip()\n",
    "            out[k] = {\"op\": \"blank\"} if sv.lower() in (\"blank\", \"tbd\") else {\n",
    "                \"op\": \"const\",\n",
    "                \"value\": sv,\n",
    "            }\n",
    "        else:\n",
    "            out[k] = {\"op\": \"blank\"}\n",
    "    return out\n",
    "\n",
    "\n",
    "def needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\", \"date_mmddyyyy\", \"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\", \"name_last_from_full\", \"money\",\n",
    "        \"membercount_from_commission\",\n",
    "    }\n",
    "\n",
    "\n",
    "def bind_sources_to_headers(headers: List[str],\n",
    "                            rule_spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    norm_map = _build_header_index(headers)\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    spec = normalize_rule_spec(rule_spec_in)\n",
    "\n",
    "    for tgt, spec in spec.items():\n",
    "        op = str(spec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            fixed[tgt] = {\"op\": \"blank\"}\n",
    "            continue\n",
    "        if not needs_source(op):\n",
    "            fixed[tgt] = spec\n",
    "            continue\n",
    "\n",
    "        src = str(spec.get(\"source\", \"\")).strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = spec\n",
    "            continue\n",
    "\n",
    "        if src in headers:\n",
    "            spec[\"source\"] = src\n",
    "        else:\n",
    "            ci = next((h for h in headers if h.lower() == src.lower()), None)\n",
    "            if ci:\n",
    "                spec[\"source\"] = ci\n",
    "            else:\n",
    "                nk = _norm_key(src)\n",
    "                if nk in norm_map:\n",
    "                    spec[\"source\"] = norm_map[nk]\n",
    "\n",
    "        fixed[tgt] = spec\n",
    "\n",
    "    return fixed\n",
    "\n",
    "\n",
    "def promote_pid_to_ptd(spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"PID\" in spec and (\"PTD\" not in spec or\n",
    "                          str(spec[\"PTD\"].get(\"op\", \"\")).lower() in (\"\", \"blank\")):\n",
    "        spec[\"PTD\"] = spec[\"PID\"]\n",
    "    return spec\n",
    "\n",
    "\n",
    "def collect_usecols(bound_spec: Dict[str, Any]) -> List[str]:\n",
    "    cols: set[str] = set()\n",
    "    for _, spec in bound_spec.items():\n",
    "        if isinstance(spec, dict) and needs_source(str(spec.get(\"op\", \"\")).strip()):\n",
    "            src = spec.get(\"source\")\n",
    "            if src:\n",
    "                cols.add(str(src))\n",
    "    return sorted(cols)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Transform (vectorized / Ray)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series):\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    s = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        parts = name.split()\n",
    "        return \" \".join(parts) if len(parts) >= 2 else name\n",
    "\n",
    "    swapped = s.where(~comma, s.map(_normalize))\n",
    "    first = swapped.str.split().str[0].fillna(\"\")\n",
    "    last = swapped.str.split().str[-1].fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,\\$]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = np.where(neg_paren, -num, num)\n",
    "    out = np.where(\n",
    "        pd.isna(num),\n",
    "        \"\",\n",
    "        np.where(pd.notnull(num), np.vectorize(lambda v: f\"{v:.2f}\")(num), \"\"),\n",
    "    )\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,\\$]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = np.where(neg_paren, -num, num)\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "\n",
    "def apply_rules(df: pd.DataFrame, bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "    spec = normalize_rule_spec(bound_spec)\n",
    "    spec = promote_pid_to_ptd(spec)\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        tspec = spec.get(tgt) or (spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not isinstance(tspec, dict):\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "        op = str(tspec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = df.get(s, empty()).astype(str)\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series(\n",
    "                [str(tspec.get(\"value\", \"\"))] * len(df),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = _to_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = _add_one_month_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[0]\n",
    "        elif op == \"name_last_from_full\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[1]\n",
    "        elif op == \"money\":\n",
    "            s = tspec.get(\"source\")\n",
    "            out[tgt] = _money_to_float_str(df.get(s, empty()))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            s = tspec.get(\"source\")\n",
    "            flags = _sign_flag_from_money(df.get(s, empty()))\n",
    "            out[tgt] = pd.Series(\n",
    "                np.where(flags.eq(\"1\"), \"1\", flags),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Ray helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def should_use_ray(n_rows: int) -> bool:\n",
    "    if ENABLE_RAY == \"on\":\n",
    "        return True\n",
    "    if ENABLE_RAY == \"off\":\n",
    "        return False\n",
    "    return n_rows >= RAY_MIN_ROWS_TO_USE\n",
    "\n",
    "\n",
    "def apply_rules_parallel(df: pd.DataFrame,\n",
    "                         bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    import ray\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(ignore_reinit_error=True, include_dashboard=False, log_to_driver=False)\n",
    "    spec_ref = ray.put(bound_spec)\n",
    "\n",
    "    @ray.remote\n",
    "    def _worker(chunk: pd.DataFrame, spec_ref):\n",
    "        return apply_rules(chunk, ray.get(spec_ref))\n",
    "\n",
    "    parts = np.array_split(df, max(1, RAY_PARTITIONS))\n",
    "    futures = [_worker.remote(part, spec_ref) for part in parts]\n",
    "    outs = ray.get(futures)\n",
    "    return pd.concat(outs, ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Manhattan Life helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def extract_manhattan_policy_plan_from_csv(csv_path: str, log: Callable[[str], None]) -> pd.DataFrame:\n",
    "    \"\"\"Read Manhattan Life raw CSV, flatten 2-row headers, return ['PolicyNumber','PlanCode']\"\"\"\n",
    "    p = str(csv_path).strip().strip('\"').strip(\"'\")\n",
    "    if p.startswith(\"/\") and not p.startswith(\"\\\\\\\\\"):\n",
    "        # normalize single-leading backslash to UNC\n",
    "        p = \"\\\\\" + p\n",
    "    log(f\"[ManhattanLife] Reading raw CSV: {p}\")\n",
    "\n",
    "    # Try two-row header first (common for Manhattan Life), then fallback\n",
    "    read_kwargs = dict(dtype=str, engine=\"python\")\n",
    "    try:\n",
    "        raw = pd.read_csv(p, header=[0, 1], encoding=\"utf-8-sig\", **read_kwargs)\n",
    "    except Exception:\n",
    "        try:\n",
    "            raw = pd.read_csv(p, header=0, encoding=\"utf-8-sig\", **read_kwargs)\n",
    "        except Exception:\n",
    "            raw = pd.read_csv(p, header=0, encoding=\"latin1\", **read_kwargs)\n",
    "\n",
    "    # ---- Flatten header names\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        flat: List[str] = []\n",
    "        for parts in raw.columns:\n",
    "            parts = [str(x).strip() for x in parts\n",
    "                     if x is not None and str(x).strip() != \"\"]\n",
    "            name = re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "            flat.append(name)\n",
    "        raw.columns = flat\n",
    "    else:\n",
    "        raw.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in raw.columns]\n",
    "\n",
    "    # Normalized header map\n",
    "    norm = {c: re.sub(r\"[^A-Za-z0-9]\", \"\", c.lower()) for c in raw.columns}\n",
    "\n",
    "    # ---- Find PlanCode (prefer headers containing both 'plan' and 'code')\n",
    "    plan_code_col: Optional[str] = None\n",
    "    for col, nc in norm.items():\n",
    "        if \"plan\" in nc and \"code\" in nc:\n",
    "            plan_code_col = col\n",
    "            break\n",
    "\n",
    "    if plan_code_col is None:\n",
    "        # last resort: a column exactly called 'plancode'\n",
    "        for col, nc in norm.items():\n",
    "            if nc == \"plancode\":\n",
    "                plan_code_col = col\n",
    "                break\n",
    "\n",
    "    # ---- Find PolicyNumber\n",
    "    # 1) exact/near matches\n",
    "    policy_col: Optional[str] = None\n",
    "    for col, nc in norm.items():\n",
    "        if (\"policy\" in nc and \"number\" in nc) or nc in (\"policynumber\", \"policyno\"):\n",
    "            policy_col = col\n",
    "            break\n",
    "\n",
    "    # 2) heuristic: any 'policy' column with the most numeric-looking values\n",
    "    if policy_col is None:\n",
    "        candidates = [c for c in raw.columns if \"policy\" in c.lower()]\n",
    "        if candidates:\n",
    "            def numeric_ratio(series: pd.Series) -> float:\n",
    "                s = series.dropna().astype(str).str.strip()\n",
    "                if len(s) == 0:\n",
    "                    return 0.0\n",
    "                m = s.str.match(r\"^\\d{5,}$\")  # mostly long numerics\n",
    "                return m.mean()\n",
    "\n",
    "            policy_col = max(candidates, key=lambda c: numeric_ratio(raw[c]))\n",
    "        # if it's clearly not numeric, fall back to first candidate anyway\n",
    "\n",
    "    if policy_col is None or plan_code_col is None:\n",
    "        raise ValueError(\n",
    "            \"Could not locate Policy/PlanCode columns.\\n\"\n",
    "            f\"Seen headers: {list(raw.columns)[:20]}\"\n",
    "        )\n",
    "\n",
    "    df2 = raw[[policy_col, plan_code_col]].copy()\n",
    "    df2.columns = [\"PolicyNumber\", \"PlanCode\"]\n",
    "    df2[\"PolicyNumber\"] = df2[\"PolicyNumber\"].astype(str).str.strip()\n",
    "    df2[\"PlanCode\"] = df2[\"PlanCode\"].astype(str).str.strip().str.upper()\n",
    "    df2 = df2[df2[\"PolicyNumber\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    log(\n",
    "        f\"[ManhattanLife] Extracted rows: {len(df2)} | \"\n",
    "        f\"cols -> PolicyNumber: {policy_col}, PlanCode: {plan_code_col}\"\n",
    "    )\n",
    "    return df2\n",
    "\n",
    "\n",
    "def match_llm_output_to_raw_counts(raw_link_df: pd.DataFrame,\n",
    "                                   llm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure the LLM output has the same number of rows per PolicyNo\n",
    "    as the raw_link_df (by repeating or trimming rows).\n",
    "    \"\"\"\n",
    "    target_counts = raw_link_df[\"PolicyNo\"].value_counts()\n",
    "    adjusted: List[pd.DataFrame] = []\n",
    "\n",
    "    for policy_no, target_n in target_counts.items():\n",
    "        block = llm_df[llm_df[\"PolicyNo\"] == policy_no]\n",
    "\n",
    "        if block.empty:\n",
    "            # No output produced for this policy_no\n",
    "            continue\n",
    "\n",
    "        if len(block) == target_n:\n",
    "            adjusted.append(block)\n",
    "            continue\n",
    "\n",
    "        if len(block) > target_n:\n",
    "            # Trim down\n",
    "            adjusted.append(block.head(target_n))\n",
    "        else:\n",
    "            # Repeat rows to reach target_n\n",
    "            repeats = (target_n // len(block)) + 1\n",
    "            expanded = pd.concat([block] * repeats, ignore_index=True)\n",
    "            adjusted.append(expanded.head(target_n))\n",
    "\n",
    "    if not adjusted:\n",
    "        return llm_df.copy()\n",
    "\n",
    "    return pd.concat(adjusted, ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Public entry point the Flask app will call\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "from manhattan_mapping import get_manhattan_mapping\n",
    "from stg_plan_mapping_min import stg_plan_mapping_min\n",
    "\n",
    "\n",
    "def log(line: str) -> None:\n",
    "    # stream logs immediately to console\n",
    "    print(line, flush=True)\n",
    "\n",
    "\n",
    "def run_llm_pipeline(\n",
    "    *,\n",
    "    issuer: str,\n",
    "    paycode: str,\n",
    "    trandate: str,\n",
    "    load_task_id: str,\n",
    "    company_issuer_id: str,\n",
    "    csv_path: str,\n",
    "    template_dir: str,\n",
    "    output_csv_name: str,\n",
    "    server_name: str,\n",
    "    database_name: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Returns output file path. Logs progress via `log(line)`.\n",
    "    `template_dir` should contain `<issuer>_prompt.txt` and `<issuer>_rules.json`.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    log(f\"Starting LLM pipeline | issuer={issuer} | csv={csv_path}\")\n",
    "\n",
    "    loader = CARRIERS.get(issuer, {}).get(\"loader\", \"csv\")\n",
    "    prompt_path = Path(template_dir) / f\"{issuer}_prompt.txt\"\n",
    "    rules_path = Path(template_dir) / f\"{issuer}_rules.json\"\n",
    "\n",
    "    if not prompt_path.exists():\n",
    "        log(f\"NOTE: prompt file not found, continuing: {prompt_path}\")\n",
    "    if not rules_path.exists():\n",
    "        log(f\"NOTE: rules file not found, continuing: {rules_path}\")\n",
    "\n",
    "    # 1) Probe headers\n",
    "    headers = _fast_read_header(csv_path, loader)\n",
    "    sig = _sig_from_cols(headers)\n",
    "    compiled_path = Path(template_dir) / f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "    # 2) LLM generate or load cached compiled rules\n",
    "    if compiled_path.exists():\n",
    "        log(f\"[Rules] Loaded cached compiled rules: {compiled_path.name}\")\n",
    "        bound_spec = json.loads(compiled_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        log(\"[Rules] Generating with LLM.\")\n",
    "        raw_spec = llm_generate_rule_spec(headers, prompt_path, rules_path)\n",
    "        raw_spec = canonicalize_spec(raw_spec)\n",
    "        bound_spec = bind_sources_to_headers(headers, raw_spec)\n",
    "        bound_spec = promote_pid_to_ptd(bound_spec)\n",
    "        compiled_path.write_text(\n",
    "            json.dumps(bound_spec, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        log(f\"[Rules] Compiled & saved: {compiled_path.name}\")\n",
    "\n",
    "    # 3) Minimal IO re-read\n",
    "    usecols = collect_usecols(bound_spec)\n",
    "    df = _read_csv_usecols(csv_path, usecols if usecols else None, loader)\n",
    "    n = len(df)\n",
    "    log(f\"[IO] Rows loaded: {n} | usecols={len(usecols)} | loader={loader}\")\n",
    "\n",
    "    # 4) Transform\n",
    "    use_ray = should_use_ray(n)\n",
    "    if use_ray:\n",
    "        log(\"[Exec] Using Ray parallel mode.\")\n",
    "        out_df = apply_rules_parallel(df, bound_spec)\n",
    "    else:\n",
    "        out_df = apply_rules(df, bound_spec)\n",
    "\n",
    "    # 5) Add constants (PlanCode column added here BEFORE enrichment)\n",
    "    out_df[\"TranDate\"] = trandate\n",
    "    out_df[\"PayCode\"] = paycode\n",
    "    out_df[\"Issuer\"] = issuer\n",
    "    out_df[\"ProductType\"] = \"\"\n",
    "    out_df[\"PlanName\"] = \"\"\n",
    "    if \"PlanCode\" not in out_df.columns:\n",
    "        out_df[\"PlanCode\"] = \"\"   # ðŸ”¹ ensure PlanCode exists prior to step 6\n",
    "    out_df[\"Note\"] = \"\"\n",
    "\n",
    "    out_df.to_csv(\"./out/base_test.csv\", index=None)\n",
    "\n",
    "    # 6) Manhattan Life enrichment (now also fills PlanCode from raw file)\n",
    "    if issuer == \"Manhattan Life\":\n",
    "        log(\"[INFO] Manhattan Life detected â€” enriching from DB (PlanCode -> map_df -> PolicyNumber -> out_df).\")\n",
    "        try:\n",
    "            # raw_link_df: PolicyNumber + PlanCode from raw file\n",
    "            raw_link_df = extract_manhattan_policy_plan_from_csv(csv_path, log)\n",
    "            r = raw_link_df.copy()\n",
    "            r[\"PolicyNo\"] = r[\"PolicyNumber\"]\n",
    "\n",
    "            # For DB, treat PlanCode as PlanId (unchanged behavior for DB layer)\n",
    "            payload_db = raw_link_df.rename(columns={\"PlanCode\": \"PlanId\"})[\n",
    "                [\"PolicyNumber\", \"PlanId\"]\n",
    "            ].copy()\n",
    "            payload_db[\"LoadTaskId\"] = load_task_id\n",
    "            payload_db = payload_db[[\"LoadTaskId\", \"PolicyNumber\", \"PlanId\"]]\n",
    "\n",
    "            inserted = stg_plan_mapping_min(\n",
    "                df=payload_db,\n",
    "                server=server_name,\n",
    "                database=database_name,\n",
    "                log=log,\n",
    "            )\n",
    "            log(f\"[ManhattanLife] STG insert-min rows: {inserted}\")\n",
    "\n",
    "            map_df = get_manhattan_mapping(\n",
    "                Load_task_id=load_task_id,\n",
    "                company_issuer_id=company_issuer_id,\n",
    "                server=server_name,\n",
    "                database=database_name,\n",
    "                log=log,\n",
    "            )\n",
    "\n",
    "            out_df_cols = [\n",
    "                \"PolicyNo\", \"PHFirst\", \"PHLast\", \"Status\", \"Issuer\", \"State\",\n",
    "                \"ProductType\", \"PlanName\", \"PlanCode\",\n",
    "                \"SubmittedDate\", \"EffectiveDate\",\n",
    "                \"TermDate\", \"Paysched\", \"PayCode\", \"WritingAgentID\", \"Premium\",\n",
    "                \"CommPrem\", \"TranDate\", \"CommReceived\", \"PTD\", \"NoPayMon\",\n",
    "                \"Membercount\", \"Note\",\n",
    "            ]\n",
    "\n",
    "            out_df2 = out_df.copy()\n",
    "\n",
    "            # Ensure PolicyNo exists for merge\n",
    "            if \"PolicyNo\" not in out_df2.columns and \"PolicyNO\" in out_df2.columns:\n",
    "                out_df2[\"PolicyNo\"] = out_df2[\"PolicyNO\"]\n",
    "\n",
    "            # Drop any existing PlanName/ProductType; we re-add below\n",
    "            out_df2 = out_df2.drop(columns=[\"PlanName\", \"ProductType\"], errors=\"ignore\")\n",
    "\n",
    "            if map_df.shape[0] == 0:\n",
    "                # No DB mapping rows â€“ just copy PlanCode from raw file, blank PlanName/ProductType\n",
    "                payload_plan = raw_link_df[[\"PolicyNumber\", \"PlanCode\"]].copy()\n",
    "                payload_plan.columns = [\"PolicyNo\", \"PlanCode\"]\n",
    "                out_df2 = out_df2.merge(payload_plan, on=\"PolicyNo\", how=\"left\")\n",
    "                out_df2[\"PlanName\"] = \"\"\n",
    "                out_df2[\"ProductType\"] = \"\"\n",
    "            else:\n",
    "                # Merge PlanName/ProductType from DB mapping\n",
    "                out_df2 = out_df2.merge(\n",
    "                    map_df[[\"PolicyNo\", \"PlanName\", \"ProductType\"]],\n",
    "                    on=\"PolicyNo\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "\n",
    "                # Bring in PlanCode from raw file\n",
    "                payload_plan = raw_link_df[[\"PolicyNumber\", \"PlanCode\"]].copy()\n",
    "                payload_plan.columns = [\"PolicyNo\", \"PlanCode_raw\"]\n",
    "                out_df2 = out_df2.merge(payload_plan, on=\"PolicyNo\", how=\"left\")\n",
    "\n",
    "                # Prefer existing PlanCode if LLM produced one, else use raw file\n",
    "                if \"PlanCode\" in out_df2.columns:\n",
    "                    out_df2[\"PlanCode\"] = out_df2[\"PlanCode\"].fillna(out_df2[\"PlanCode_raw\"])\n",
    "                else:\n",
    "                    out_df2[\"PlanCode\"] = out_df2[\"PlanCode_raw\"]\n",
    "\n",
    "                out_df2 = out_df2.drop(columns=[\"PlanCode_raw\"])\n",
    "\n",
    "            out_df2 = out_df2[out_df_cols].fillna(\"\")\n",
    "            out_df = match_llm_output_to_raw_counts(r, out_df2)\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"[WARN] Manhattan Life enrichment failed: {e}\")\n",
    "            if \"PlanName\" not in out_df.columns:\n",
    "                out_df[\"PlanName\"] = \"\"\n",
    "            if \"ProductType\" not in out_df.columns:\n",
    "                out_df[\"ProductType\"] = \"\"\n",
    "            if \"PlanCode\" not in out_df.columns:\n",
    "                out_df[\"PlanCode\"] = \"\"\n",
    "\n",
    "    # 7) Write output file\n",
    "    input_path = Path(csv_path)\n",
    "    out_dir = input_path.parent\n",
    "    out_path = out_dir / f\"{output_csv_name}.csv\"\n",
    "\n",
    "    try:\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        log(f\"[Output] file written sucessfully: {out_path}\")\n",
    "    except Exception as e:\n",
    "        log(f\"[ERROR] Failed to write output file: {e}\")\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    log(f\"Completed: {out_path.as_posix()} (elapsed {elapsed:.2f}s)\")\n",
    "\n",
    "    return out_path.as_posix()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
