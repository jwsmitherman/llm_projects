{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bc08f",
   "metadata": {},
   "source": [
    "# ✅ LLM Processor Test Notebook\n",
    "This notebook is ready for you to paste the full updated `llm_processor_dbconn.py` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#   FULL llm_processor_dbconn.py — DROP-IN NOTEBOOK VERSION\n",
    "#   Complete, single-cell, self-contained, PlanCode fully added\n",
    "#   Includes:\n",
    "#     ✔ Ray support\n",
    "#     ✔ should_use_ray()\n",
    "#     ✔ Manhattan Life 2-row header handling\n",
    "#     ✔ PlanCode extraction + merging\n",
    "#     ✔ LLM rule generation\n",
    "#     ✔ Transformation engine\n",
    "#     ✔ match_llm_output_to_raw_counts\n",
    "# ================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, re, json, hashlib, time, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ---------- LangChain Azure OpenAI ----------\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except:\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# ---------- Perf Config ----------\n",
    "ENABLE_RAY = os.getenv(\"ENABLE_RAY\", \"auto\")  # auto / on / off\n",
    "RAY_PARTITIONS = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve()\n",
    "OUT_DIR = Path(BASE_DIR / \"outbound\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Columns (PlanCode added) ----------\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNO\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\n",
    "    \"ProductType\",\"PlanName\",\"PlanCode\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"Paysched\",\n",
    "    \"PayCode\",\"WritingAgentID\",\"Premium\",\"CommPrem\",\n",
    "    \"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\n",
    "    \"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = [\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Return strict JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Utility ----------\n",
    "def _norm_key(s): return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "def _sig_from_cols(cols): return hashlib.sha1((\"||\".join(cols)).encode()).hexdigest()[:12]\n",
    "def _build_header_index(cols): return {_norm_key(c): c for c in cols}\n",
    "\n",
    "# ---------- Loader ----------\n",
    "CARRIERS = {\n",
    "    \"Manhattan Life\": {\"loader\": \"two_header\"},\n",
    "    \"Molina\": {\"loader\": \"csv\"},\n",
    "    \"Ameritas\": {\"loader\": \"csv\"}\n",
    "}\n",
    "\n",
    "def _fast_read_header(path, loader):\n",
    "    if loader == \"csv\":\n",
    "        return list(pd.read_csv(path, nrows=0, dtype=str).columns)\n",
    "\n",
    "    # two_header\n",
    "    tmp = pd.read_csv(path, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "    cols = []\n",
    "    last = \"\"\n",
    "    for a,b in zip(top,bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if a: last=a\n",
    "        name = f\"{last} {b}\".strip() if b else last\n",
    "        name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", name.replace(\" \", \"_\"))\n",
    "        cols.append(name)\n",
    "    return cols\n",
    "\n",
    "def _read_csv_usecols(path, usecols, loader):\n",
    "    if loader==\"csv\":\n",
    "        return pd.read_csv(path, dtype=str, usecols=usecols).fillna(\"\")\n",
    "\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "    cols=[]\n",
    "    last=\"\"\n",
    "    for a,b in zip(top,bottom):\n",
    "        a=str(a).strip(); b=str(b).strip()\n",
    "        if a: last=a\n",
    "        name = f\"{last} {b}\".strip() if b else last\n",
    "        name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", name.replace(\" \", \"_\"))\n",
    "        cols.append(name)\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns=cols\n",
    "    if usecols: df=df[[c for c in usecols if c in df.columns]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# ---------- LLM ----------\n",
    "def build_llm():\n",
    "    return AzureChatOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\",\"https://YOUR-ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\",\"\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\",\"2024-12-01-preview\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\",\"gpt-4o-mini\"),\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "def llm_generate_rule_spec(headers, prompt_path, rules_path):\n",
    "    llm = build_llm()\n",
    "    payload={\n",
    "        \"RequiredFields\": FINAL_COLUMNS+[\"PID\"],\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": prompt_path.read_text() if prompt_path.exists() else \"\",\n",
    "        \"ExtraPrompt\": rules_path.read_text() if rules_path.exists() else \"\",\n",
    "        \"OutputFormat\": \"Return strict JSON.\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":json.dumps(payload)}\n",
    "    ])\n",
    "    return json.loads(resp.content)\n",
    "\n",
    "# ---------- Spec Binding ----------\n",
    "CANON={_norm_key(k):k for k in FINAL_COLUMNS+[\"PID\"]}\n",
    "\n",
    "def canonicalize_spec(spec):\n",
    "    fixed={}\n",
    "    for k,v in spec.items():\n",
    "        fixed[CANON.get(_norm_key(k),k)] = v\n",
    "    for req in FINAL_COLUMNS:\n",
    "        if req not in fixed:\n",
    "            fixed[req]={\"op\":\"blank\"}\n",
    "    return fixed\n",
    "\n",
    "def needs_source(op):\n",
    "    return op in [\"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "                  \"name_first_from_full\",\"name_last_from_full\",\n",
    "                  \"money\",\"membercount_from_commission\"]\n",
    "\n",
    "def bind_sources_to_headers(headers, spec):\n",
    "    norm=_build_header_index(headers)\n",
    "    out={}\n",
    "    for tgt,s in spec.items():\n",
    "        op=s.get(\"op\")\n",
    "        if not needs_source(op):\n",
    "            out[tgt]=s; continue\n",
    "        src=s.get(\"source\",\"\")\n",
    "        if src in headers:\n",
    "            out[tgt]=s; continue\n",
    "        nk=_norm_key(src)\n",
    "        if nk in norm:\n",
    "            s[\"source\"]=norm[nk]\n",
    "        out[tgt]=s\n",
    "    return out\n",
    "\n",
    "def promote_pid_to_ptd(spec):\n",
    "    if \"PID\" in spec and (\"PTD\" not in spec or spec[\"PTD\"].get(\"op\")==\"blank\"):\n",
    "        spec[\"PTD\"]=spec[\"PID\"]\n",
    "    return spec\n",
    "\n",
    "def collect_usecols(spec):\n",
    "    cols=set()\n",
    "    for tgt,s in spec.items():\n",
    "        if needs_source(s.get(\"op\",\"\")):\n",
    "            cols.add(s.get(\"source\"))\n",
    "    return [c for c in cols if c]\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "def _to_mmddyyyy(s):\n",
    "    dt=pd.to_datetime(s,errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s):\n",
    "    dt=pd.to_datetime(s,errors=\"coerce\")\n",
    "    return (dt+relativedelta(months=1)).dt.strftime(\"%m/%d/%Y\").fillna(\"\")\n",
    "\n",
    "def _name_first_last(series):\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    parts = s.str.split()\n",
    "    return parts.str[0].fillna(\"\"), parts.str[-1].fillna(\"\")\n",
    "\n",
    "def apply_rules(df, spec):\n",
    "    out={}\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        rule=spec.get(tgt,{\"op\":\"blank\"})\n",
    "        op=rule.get(\"op\")\n",
    "        if op==\"copy\":\n",
    "            out[tgt]=df[rule[\"source\"]]\n",
    "        elif op==\"const\":\n",
    "            out[tgt]=rule[\"value\"]\n",
    "        elif op==\"date_mmddyyyy\":\n",
    "            out[tgt]=_to_mmddyyyy(df[rule[\"source\"]])\n",
    "        elif op==\"date_plus_1m_mmddyyyy\":\n",
    "            out[tgt]=_add_one_month_mmddyyyy(df[rule[\"source\"]])\n",
    "        elif op==\"name_first_from_full\":\n",
    "            out[tgt]=_name_first_last(df[rule[\"source\"]])[0]\n",
    "        elif op==\"name_last_from_full\":\n",
    "            out[tgt]=_name_first_last(df[rule[\"source\"]])[1]\n",
    "        elif op==\"money\":\n",
    "            out[tgt]=df[rule[\"source\"]].astype(str)\n",
    "        elif op==\"membercount_from_commission\":\n",
    "            s=df[rule[\"source\"]].astype(str)\n",
    "            out[tgt]=np.where(s.str.contains(\"-\"),\"-1\",\"1\")\n",
    "        else:\n",
    "            out[tgt]=\"\"\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# ---------- Ray ----------\n",
    "def should_use_ray(n):\n",
    "    if ENABLE_RAY==\"on\": return True\n",
    "    if ENABLE_RAY==\"off\": return False\n",
    "    return n>=RAY_MIN_ROWS_TO_USE\n",
    "\n",
    "def apply_rules_parallel(df, spec):\n",
    "    import ray\n",
    "    if not ray.is_initialized(): ray.init(ignore_reinit_error=True)\n",
    "    spec_ref=ray.put(spec)\n",
    "\n",
    "    @ray.remote\n",
    "    def worker(chunk, spec_ref): return apply_rules(chunk, ray.get(spec_ref))\n",
    "    parts=np.array_split(df, RAY_PARTITIONS)\n",
    "    return pd.concat(ray.get([worker.remote(p,spec_ref) for p in parts]))\n",
    "\n",
    "# ---------- Manhattan Life ----------\n",
    "def extract_manhattan_policy_plan_from_csv(path, log):\n",
    "    df=pd.read_csv(path, header=[0,1]).fillna(\"\")\n",
    "    flat=[]\n",
    "    for a,b in df.columns:\n",
    "        name=f\"{str(a).strip()} {str(b).strip()}\".strip()\n",
    "        name=re.sub(r\"[^A-Za-z0-9_]\",\"_\",name.replace(\" \",\"_\"))\n",
    "        flat.append(name)\n",
    "    df.columns=flat\n",
    "\n",
    "    pc=[c for c in df.columns if \"plan\" in c.lower() and \"code\" in c.lower()]\n",
    "    po=[c for c in df.columns if \"policy\" in c.lower() or \"case_number\" in c.lower()]\n",
    "    if not pc or not po:\n",
    "        raise ValueError(\"Could not locate PlanCode or PolicyNumber\")\n",
    "\n",
    "    out=df[[po[0], pc[0]]].copy()\n",
    "    out.columns=[\"PolicyNumber\",\"PlanCode\"]\n",
    "    out[\"PolicyNumber\"]=out[\"PolicyNumber\"].astype(str).str.strip()\n",
    "    out[\"PlanCode\"]=out[\"PlanCode\"].astype(str).str.strip().str.upper()\n",
    "    out[\"PolicyNo\"]=out[\"PolicyNumber\"]\n",
    "    return out[out[\"PolicyNumber\"]!=\"\"]\n",
    "\n",
    "def match_llm_output_to_raw_counts(raw, llm):\n",
    "    counts=raw[\"PolicyNo\"].value_counts()\n",
    "    dfs=[]\n",
    "    for pol,n in counts.items():\n",
    "        block=llm[llm[\"PolicyNo\"]==pol]\n",
    "        if len(block)==0: continue\n",
    "        if len(block)>=n:\n",
    "            dfs.append(block.head(n))\n",
    "        else:\n",
    "            dfs.append(pd.concat([block]*((n//len(block))+1)).head(n))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "# DB stubs (you will replace with real code)\n",
    "def get_manhattan_mapping(Load_task_id, company_issuer_id, server, database, log):\n",
    "    return pd.DataFrame(columns=[\"PolicyNo\",\"PlanName\",\"ProductType\"])\n",
    "\n",
    "def stg_plan_mapping_min(df, server, database, log): return len(df)\n",
    "\n",
    "# ---------- Pipeline ----------\n",
    "def run_llm_pipeline(\n",
    "    issuer, paycode, trandate, load_task_id, company_issuer_id,\n",
    "    csv_path, template_dir, output_csv_name,\n",
    "    server_name, database_name,\n",
    "    log=print):\n",
    "\n",
    "    loader=CARRIERS.get(issuer,{}).get(\"loader\",\"csv\")\n",
    "    headers=_fast_read_header(csv_path, loader)\n",
    "    sig=_sig_from_cols(headers)\n",
    "\n",
    "    prompt_path=Path(template_dir)/f\"{issuer}_prompt.txt\"\n",
    "    rules_path=Path(template_dir)/f\"{issuer}_rules.json\"\n",
    "    compiled_path=Path(template_dir)/f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "    if compiled_path.exists():\n",
    "        spec=json.loads(compiled_path.read_text())\n",
    "    else:\n",
    "        raw=llm_generate_rule_spec(headers,prompt_path,rules_path)\n",
    "        raw=canonicalize_spec(raw)\n",
    "        raw=bind_sources_to_headers(headers,raw)\n",
    "        raw=promote_pid_to_ptd(raw)\n",
    "        compiled_path.write_text(json.dumps(raw,indent=2))\n",
    "        spec=raw\n",
    "\n",
    "    usecols=collect_usecols(spec)\n",
    "    df=_read_csv_usecols(csv_path, usecols, loader)\n",
    "\n",
    "    if should_use_ray(len(df)):\n",
    "        out=apply_rules_parallel(df,spec)\n",
    "    else:\n",
    "        out=apply_rules(df,spec)\n",
    "\n",
    "    out[\"TranDate\"]=trandate\n",
    "    out[\"PayCode\"]=paycode\n",
    "    out[\"Issuer\"]=issuer\n",
    "    if \"PlanCode\" not in out: out[\"PlanCode\"]=\"\"\n",
    "\n",
    "    if issuer==\"Manhattan Life\":\n",
    "        log(\"Enriching Manhattan Life…\")\n",
    "        raw_link=extract_manhattan_policy_plan_from_csv(csv_path,log)\n",
    "        map_df=get_manhattan_mapping(load_task_id,company_issuer_id,server_name,database_name,log)\n",
    "\n",
    "        merged=out.merge(raw_link[[\"PolicyNo\",\"PlanCode\"]],on=\"PolicyNo\",how=\"left\")\n",
    "        merged[\"PlanCode\"]=merged[\"PlanCode_x\"].fillna(merged[\"PlanCode_y\"])\n",
    "        merged=merged.drop(columns=[\"PlanCode_x\",\"PlanCode_y\"])\n",
    "        out=match_llm_output_to_raw_counts(raw_link, merged)\n",
    "\n",
    "    out_file=Path(csv_path).parent/f\"{output_csv_name}.csv\"\n",
    "    out.to_csv(out_file,index=False)\n",
    "    log(f\"Done → {out_file}\")\n",
    "    return out_file\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "#   EXAMPLE INPUTS FOR TESTING\n",
    "# ===================================================\n",
    "issuer=\"Manhattan Life\"\n",
    "paycode=\"Default\"\n",
    "trandate=\"2025-11-12\"\n",
    "load_task_id=\"11497\"\n",
    "company_issuer_id=\"3205\"\n",
    "\n",
    "csv_path = r\".\\inbound\\raw.csv\"           # ✔ update\n",
    "template_dir = r\".\\carrier_prompts\"       # ✔ update\n",
    "\n",
    "server_name=\"NGCS\"\n",
    "database_name=\"NGCS\"\n",
    "\n",
    "# ===================================================\n",
    "#   RUN PIPELINE\n",
    "# =============================import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, re, json, hashlib, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. CONFIG (match what you showed in the screenshot)\n",
    "# ------------------------------------------------------------\n",
    "issuer            = \"Manhattan Life\"\n",
    "paycode           = \"Default\"\n",
    "trandate          = \"2025-11-12\"\n",
    "load_task_id      = \"11497\"\n",
    "company_issuer_id = \"3205\"\n",
    "\n",
    "csv_path     = r\".\\inbound\\raw - 9836c995-9e25-45ae-af39-3b4cc8ac1bbd.csv\"\n",
    "template_dir = r\".\\carrier_prompts\"\n",
    "\n",
    "server_name   = \"QWSD8SQLB401.nguottit.com\"   # or \"NGCS\" etc.\n",
    "database_name = \"NGCS\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. CORE CONSTANTS (only change: PlanCode added to FINAL_COLUMNS)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "CARRIERS = {\n",
    "    \"Molina\": {\"loader\": \"csv\"},\n",
    "    \"Ameritas\": {\"loader\": \"csv\"},\n",
    "    \"Manhattan Life\": {\"loader\": \"two_header\"},\n",
    "}\n",
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNO\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\n",
    "    \"ProductType\",\"PlanName\",\"PlanCode\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"Paysched\",\n",
    "    \"PayCode\",\"WritingAgentID\",\"Premium\",\"CommPrem\",\n",
    "    \"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = [\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. SMALL UTILS (same as processor.py)\n",
    "# ------------------------------------------------------------\n",
    "def _norm_key(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "\n",
    "def _sig_from_cols(cols: List[str]) -> str:\n",
    "    joined = \"||\".join(map(str, cols))\n",
    "    return hashlib.sha1(joined.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def _build_header_index(cols: List[str]) -> Dict[str, str]:\n",
    "    return {_norm_key(h): h for h in cols}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. HEADER PROBE + CSV READER (csv vs two_header)\n",
    "# ------------------------------------------------------------\n",
    "def _fast_read_header(path: str, loader: str) -> List[str]:\n",
    "    if loader == \"csv\":\n",
    "        dfo = pd.read_csv(path, nrows=0, dtype=str)\n",
    "        return list(dfo.columns)\n",
    "\n",
    "    # two_header: first 2 rows\n",
    "    probe = pd.read_csv(path, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = probe.iloc[0].tolist(), probe.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    return cols\n",
    "\n",
    "def _read_csv_usecols(path: str,\n",
    "                      usecols: Optional[List[str]],\n",
    "                      loader: str) -> pd.DataFrame:\n",
    "    if loader == \"csv\":\n",
    "        return pd.read_csv(path, dtype=str, usecols=usecols if usecols else None).fillna(\"\")\n",
    "\n",
    "    # two_header\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    if usecols:\n",
    "        df = df[[c for c in usecols if c in df.columns]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. RULE SPEC NORMALIZATION + BINDING  (fixes your list .get error)\n",
    "# ------------------------------------------------------------\n",
    "_CANON = {_norm_key(k): k for k in FINAL_COLUMNS + [\"PID\"]}\n",
    "\n",
    "def canonicalize_spec(spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        nk = _norm_key(k)\n",
    "        fixed[_CANON.get(nk, k)] = v\n",
    "    for req in FINAL_COLUMNS:\n",
    "        if req not in fixed and (req != \"PTD\" or \"PID\" not in fixed):\n",
    "            fixed[req] = {\"op\": \"blank\"}\n",
    "    return fixed\n",
    "\n",
    "def normalize_rule_spec(spec_in: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            sv = v.strip()\n",
    "            out[k] = {\"op\": \"blank\"} if sv.lower() in (\"blank\", \"tbd\") else {\n",
    "                \"op\": \"const\",\n",
    "                \"value\": sv,\n",
    "            }\n",
    "        else:\n",
    "            out[k] = {\"op\": \"blank\"}\n",
    "    return out\n",
    "\n",
    "def needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\n",
    "        \"money\",\"membercount_from_commission\",\n",
    "    }\n",
    "\n",
    "def bind_sources_to_headers(headers: List[str],\n",
    "                            rule_spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    norm_map = _build_header_index(headers)\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    spec = normalize_rule_spec(rule_spec_in)   # <-- IMPORTANT: avoids list.get error\n",
    "\n",
    "    for tgt, s in spec.items():\n",
    "        op = str(s.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            fixed[tgt] = {\"op\": \"blank\"}\n",
    "            continue\n",
    "        if not needs_source(op):\n",
    "            fixed[tgt] = s\n",
    "            continue\n",
    "\n",
    "        src = str(s.get(\"source\", \"\")).strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = s\n",
    "            continue\n",
    "\n",
    "        if src in headers:\n",
    "            s[\"source\"] = src\n",
    "        else:\n",
    "            ci = next((h for h in headers if h.lower() == src.lower()), None)\n",
    "            if ci:\n",
    "                s[\"source\"] = ci\n",
    "            else:\n",
    "                nk = _norm_key(src)\n",
    "                if nk in norm_map:\n",
    "                    s[\"source\"] = norm_map[nk]\n",
    "\n",
    "        fixed[tgt] = s\n",
    "\n",
    "    return fixed\n",
    "\n",
    "def promote_pid_to_ptd(spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"PID\" in spec and (\"PTD\" not in spec or\n",
    "                          str(spec[\"PTD\"].get(\"op\", \"\")).lower() in (\"\", \"blank\")):\n",
    "        spec[\"PTD\"] = spec[\"PID\"]\n",
    "    return spec\n",
    "\n",
    "def collect_usecols(bound_spec: Dict[str, Any]) -> List[str]:\n",
    "    cols: set[str] = set()\n",
    "    for _, spec in bound_spec.items():\n",
    "        if isinstance(spec, dict) and needs_source(str(spec.get(\"op\", \"\")).strip()):\n",
    "            src = spec.get(\"source\")\n",
    "            if src:\n",
    "                cols.add(str(src))\n",
    "    return sorted(cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. TRANSFORM FUNCTIONS + apply_rules / Ray\n",
    "# ------------------------------------------------------------\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series):\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    first = s.str.split().str[0].fillna(\"\")\n",
    "    last  = s.str.split().str[-1].fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def apply_rules(df: pd.DataFrame, bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = bound_spec.get(tgt, {\"op\": \"blank\"})\n",
    "        if not isinstance(spec, dict):\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "        op = str(spec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = df.get(s, empty()).astype(str)\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series(\n",
    "                [str(spec.get(\"value\", \"\"))] * len(df),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _to_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _add_one_month_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[0]\n",
    "        elif op == \"name_last_from_full\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[1]\n",
    "        elif op == \"money\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = df.get(s, empty()).astype(str)\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            s = spec.get(\"source\")\n",
    "            vals = df.get(s, empty()).astype(str)\n",
    "            out[tgt] = pd.Series(\n",
    "                np.where(vals.str.contains(\"-\"), \"-1\", \"1\"),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "ENABLE_RAY          = os.getenv(\"ENABLE_RAY\", \"auto\")\n",
    "RAY_PARTITIONS      = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "def should_use_ray(n_rows: int) -> bool:\n",
    "    if ENABLE_RAY == \"on\":\n",
    "        return True\n",
    "    if ENABLE_RAY == \"off\":\n",
    "        return False\n",
    "    return n_rows >= RAY_MIN_ROWS_TO_USE\n",
    "\n",
    "def apply_rules_parallel(df: pd.DataFrame,\n",
    "                         bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    import ray\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(ignore_reinit_error=True, include_dashboard=False, log_to_driver=False)\n",
    "    spec_ref = ray.put(bound_spec)\n",
    "\n",
    "    @ray.remote\n",
    "    def _worker(chunk: pd.DataFrame, spec_ref):\n",
    "        return apply_rules(chunk, ray.get(spec_ref))\n",
    "\n",
    "    parts = np.array_split(df, max(1, RAY_PARTITIONS))\n",
    "    futures = [_worker.remote(part, spec_ref) for part in parts]\n",
    "    outs = ray.get(futures)\n",
    "    return pd.concat(outs, ignore_index=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. MINI-PIPELINE (everything up to BEFORE Manhattan enrichment)\n",
    "# ------------------------------------------------------------\n",
    "loader      = CARRIERS.get(issuer, {}).get(\"loader\", \"csv\")\n",
    "headers     = _fast_read_header(csv_path, loader)\n",
    "sig         = _sig_from_cols(headers)\n",
    "\n",
    "prompt_path   = Path(template_dir) / f\"{issuer}_prompt.txt\"\n",
    "rules_path    = Path(template_dir) / f\"{issuer}_rules.json\"\n",
    "compiled_path = Path(template_dir) / f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "# --- load or build rules ---\n",
    "if compiled_path.exists():\n",
    "    bound_spec = json.loads(compiled_path.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    # NOTE: in this notebook, you can also just paste an already-built rules JSON here\n",
    "    raise RuntimeError(\"Compiled rules file not found; run main processor once to generate it.\")\n",
    "\n",
    "# Make sure spec structure is normalized like in processor.py\n",
    "bound_spec = canonicalize_spec(bound_spec)\n",
    "bound_spec = bind_sources_to_headers(headers, bound_spec)\n",
    "bound_spec = promote_pid_to_ptd(bound_spec)\n",
    "\n",
    "usecols = collect_usecols(bound_spec)\n",
    "df = _read_csv_usecols(csv_path, usecols if usecols else None, loader)\n",
    "print(f\"Rows loaded: {len(df)}, usecols: {usecols}\")\n",
    "\n",
    "if should_use_ray(len(df)):\n",
    "    out_df = apply_rules_parallel(df, bound_spec)\n",
    "else:\n",
    "    out_df = apply_rules(df, bound_spec)\n",
    "\n",
    "# ---- ADD CONSTANTS + PLANCODE BEFORE MANHATTAN ENRICHMENT ----\n",
    "out_df[\"TranDate\"] = trandate\n",
    "out_df[\"PayCode\"]  = paycode\n",
    "out_df[\"Issuer\"]   = issuer\n",
    "out_df[\"ProductType\"] = \"\"\n",
    "out_df[\"PlanName\"]    = \"\"\n",
    "if \"PlanCode\" not in out_df.columns:\n",
    "    out_df[\"PlanCode\"] = \"\"      # <-- PlanCode guaranteed BEFORE #6\n",
    "out_df[\"Note\"] = \"\"\n",
    "\n",
    "print(out_df.head())\n",
    "print(out_df[[\"PolicyNO\",\"PlanCode\"]].head(20))\n",
    "======================\n",
    "out = run_llm_pipeline(\n",
    "    issuer=issuer,\n",
    "    paycode=paycode,\n",
    "    trandate=trandate,\n",
    "    load_task_id=load_task_id,\n",
    "    company_issuer_id=company_issuer_id,\n",
    "    csv_path=csv_path,\n",
    "    template_dir=template_dir,\n",
    "    output_csv_name=\"manhattan_life_test_output\",\n",
    "    server_name=server_name,\n",
    "    database_name=database_name,\n",
    ")\n",
    "\n",
    "out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
