{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bc08f",
   "metadata": {},
   "source": [
    "# ✅ LLM Processor Test Notebook\n",
    "This notebook is ready for you to paste the full updated `llm_processor_dbconn.py` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#   FULL llm_processor_dbconn.py — DROP-IN NOTEBOOK VERSION\n",
    "#   Complete, single-cell, self-contained, PlanCode fully added\n",
    "#   Includes:\n",
    "#     ✔ Ray support\n",
    "#     ✔ should_use_ray()\n",
    "#     ✔ Manhattan Life 2-row header handling\n",
    "#     ✔ PlanCode extraction + merging\n",
    "#     ✔ LLM rule generation\n",
    "#     ✔ Transformation engine\n",
    "#     ✔ match_llm_output_to_raw_counts\n",
    "# ================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, re, json, hashlib, time, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ---------- LangChain Azure OpenAI ----------\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except:\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# ---------- Perf Config ----------\n",
    "ENABLE_RAY = os.getenv(\"ENABLE_RAY\", \"auto\")  # auto / on / off\n",
    "RAY_PARTITIONS = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve()\n",
    "OUT_DIR = Path(BASE_DIR / \"outbound\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Columns (PlanCode added) ----------\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNO\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\n",
    "    \"ProductType\",\"PlanName\",\"PlanCode\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"Paysched\",\n",
    "    \"PayCode\",\"WritingAgentID\",\"Premium\",\"CommPrem\",\n",
    "    \"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\n",
    "    \"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = [\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Return strict JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Utility ----------\n",
    "def _norm_key(s): return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "def _sig_from_cols(cols): return hashlib.sha1((\"||\".join(cols)).encode()).hexdigest()[:12]\n",
    "def _build_header_index(cols): return {_norm_key(c): c for c in cols}\n",
    "\n",
    "# ---------- Loader ----------\n",
    "CARRIERS = {\n",
    "    \"Manhattan Life\": {\"loader\": \"two_header\"},\n",
    "    \"Molina\": {\"loader\": \"csv\"},\n",
    "    \"Ameritas\": {\"loader\": \"csv\"}\n",
    "}\n",
    "\n",
    "def _fast_read_header(path, loader):\n",
    "    if loader == \"csv\":\n",
    "        return list(pd.read_csv(path, nrows=0, dtype=str).columns)\n",
    "\n",
    "    # two_header\n",
    "    tmp = pd.read_csv(path, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "    cols = []\n",
    "    last = \"\"\n",
    "    for a,b in zip(top,bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if a: last=a\n",
    "        name = f\"{last} {b}\".strip() if b else last\n",
    "        name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", name.replace(\" \", \"_\"))\n",
    "        cols.append(name)\n",
    "    return cols\n",
    "\n",
    "def _read_csv_usecols(path, usecols, loader):\n",
    "    if loader==\"csv\":\n",
    "        return pd.read_csv(path, dtype=str, usecols=usecols).fillna(\"\")\n",
    "\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "    cols=[]\n",
    "    last=\"\"\n",
    "    for a,b in zip(top,bottom):\n",
    "        a=str(a).strip(); b=str(b).strip()\n",
    "        if a: last=a\n",
    "        name = f\"{last} {b}\".strip() if b else last\n",
    "        name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", name.replace(\" \", \"_\"))\n",
    "        cols.append(name)\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns=cols\n",
    "    if usecols: df=df[[c for c in usecols if c in df.columns]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# ---------- LLM ----------\n",
    "def build_llm():\n",
    "    return AzureChatOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\",\"https://YOUR-ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\",\"\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\",\"2024-12-01-preview\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\",\"gpt-4o-mini\"),\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "def llm_generate_rule_spec(headers, prompt_path, rules_path):\n",
    "    llm = build_llm()\n",
    "    payload={\n",
    "        \"RequiredFields\": FINAL_COLUMNS+[\"PID\"],\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": prompt_path.read_text() if prompt_path.exists() else \"\",\n",
    "        \"ExtraPrompt\": rules_path.read_text() if rules_path.exists() else \"\",\n",
    "        \"OutputFormat\": \"Return strict JSON.\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":json.dumps(payload)}\n",
    "    ])\n",
    "    return json.loads(resp.content)\n",
    "\n",
    "# ---------- Spec Binding ----------\n",
    "CANON={_norm_key(k):k for k in FINAL_COLUMNS+[\"PID\"]}\n",
    "\n",
    "def canonicalize_spec(spec):\n",
    "    fixed={}\n",
    "    for k,v in spec.items():\n",
    "        fixed[CANON.get(_norm_key(k),k)] = v\n",
    "    for req in FINAL_COLUMNS:\n",
    "        if req not in fixed:\n",
    "            fixed[req]={\"op\":\"blank\"}\n",
    "    return fixed\n",
    "\n",
    "def needs_source(op):\n",
    "    return op in [\"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "                  \"name_first_from_full\",\"name_last_from_full\",\n",
    "                  \"money\",\"membercount_from_commission\"]\n",
    "\n",
    "def bind_sources_to_headers(headers, spec):\n",
    "    norm=_build_header_index(headers)\n",
    "    out={}\n",
    "    for tgt,s in spec.items():\n",
    "        op=s.get(\"op\")\n",
    "        if not needs_source(op):\n",
    "            out[tgt]=s; continue\n",
    "        src=s.get(\"source\",\"\")\n",
    "        if src in headers:\n",
    "            out[tgt]=s; continue\n",
    "        nk=_norm_key(src)\n",
    "        if nk in norm:\n",
    "            s[\"source\"]=norm[nk]\n",
    "        out[tgt]=s\n",
    "    return out\n",
    "\n",
    "def promote_pid_to_ptd(spec):\n",
    "    if \"PID\" in spec and (\"PTD\" not in spec or spec[\"PTD\"].get(\"op\")==\"blank\"):\n",
    "        spec[\"PTD\"]=spec[\"PID\"]\n",
    "    return spec\n",
    "\n",
    "def collect_usecols(spec):\n",
    "    cols=set()\n",
    "    for tgt,s in spec.items():\n",
    "        if needs_source(s.get(\"op\",\"\")):\n",
    "            cols.add(s.get(\"source\"))\n",
    "    return [c for c in cols if c]\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "def _to_mmddyyyy(s):\n",
    "    dt=pd.to_datetime(s,errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s):\n",
    "    dt=pd.to_datetime(s,errors=\"coerce\")\n",
    "    return (dt+relativedelta(months=1)).dt.strftime(\"%m/%d/%Y\").fillna(\"\")\n",
    "\n",
    "def _name_first_last(series):\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    parts = s.str.split()\n",
    "    return parts.str[0].fillna(\"\"), parts.str[-1].fillna(\"\")\n",
    "\n",
    "def apply_rules(df, spec):\n",
    "    out={}\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        rule=spec.get(tgt,{\"op\":\"blank\"})\n",
    "        op=rule.get(\"op\")\n",
    "        if op==\"copy\":\n",
    "            out[tgt]=df[rule[\"source\"]]\n",
    "        elif op==\"const\":\n",
    "            out[tgt]=rule[\"value\"]\n",
    "        elif op==\"date_mmddyyyy\":\n",
    "            out[tgt]=_to_mmddyyyy(df[rule[\"source\"]])\n",
    "        elif op==\"date_plus_1m_mmddyyyy\":\n",
    "            out[tgt]=_add_one_month_mmddyyyy(df[rule[\"source\"]])\n",
    "        elif op==\"name_first_from_full\":\n",
    "            out[tgt]=_name_first_last(df[rule[\"source\"]])[0]\n",
    "        elif op==\"name_last_from_full\":\n",
    "            out[tgt]=_name_first_last(df[rule[\"source\"]])[1]\n",
    "        elif op==\"money\":\n",
    "            out[tgt]=df[rule[\"source\"]].astype(str)\n",
    "        elif op==\"membercount_from_commission\":\n",
    "            s=df[rule[\"source\"]].astype(str)\n",
    "            out[tgt]=np.where(s.str.contains(\"-\"),\"-1\",\"1\")\n",
    "        else:\n",
    "            out[tgt]=\"\"\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# ---------- Ray ----------\n",
    "def should_use_ray(n):\n",
    "    if ENABLE_RAY==\"on\": return True\n",
    "    if ENABLE_RAY==\"off\": return False\n",
    "    return n>=RAY_MIN_ROWS_TO_USE\n",
    "\n",
    "def apply_rules_parallel(df, spec):\n",
    "    import ray\n",
    "    if not ray.is_initialized(): ray.init(ignore_reinit_error=True)\n",
    "    spec_ref=ray.put(spec)\n",
    "\n",
    "    @ray.remote\n",
    "    def worker(chunk, spec_ref): return apply_rules(chunk, ray.get(spec_ref))\n",
    "    parts=np.array_split(df, RAY_PARTITIONS)\n",
    "    return pd.concat(ray.get([worker.remote(p,spec_ref) for p in parts]))\n",
    "\n",
    "# ---------- Manhattan Life ----------\n",
    "def extract_manhattan_policy_plan_from_csv(path, log):\n",
    "    df=pd.read_csv(path, header=[0,1]).fillna(\"\")\n",
    "    flat=[]\n",
    "    for a,b in df.columns:\n",
    "        name=f\"{str(a).strip()} {str(b).strip()}\".strip()\n",
    "        name=re.sub(r\"[^A-Za-z0-9_]\",\"_\",name.replace(\" \",\"_\"))\n",
    "        flat.append(name)\n",
    "    df.columns=flat\n",
    "\n",
    "    pc=[c for c in df.columns if \"plan\" in c.lower() and \"code\" in c.lower()]\n",
    "    po=[c for c in df.columns if \"policy\" in c.lower() or \"case_number\" in c.lower()]\n",
    "    if not pc or not po:\n",
    "        raise ValueError(\"Could not locate PlanCode or PolicyNumber\")\n",
    "\n",
    "    out=df[[po[0], pc[0]]].copy()\n",
    "    out.columns=[\"PolicyNumber\",\"PlanCode\"]\n",
    "    out[\"PolicyNumber\"]=out[\"PolicyNumber\"].astype(str).str.strip()\n",
    "    out[\"PlanCode\"]=out[\"PlanCode\"].astype(str).str.strip().str.upper()\n",
    "    out[\"PolicyNo\"]=out[\"PolicyNumber\"]\n",
    "    return out[out[\"PolicyNumber\"]!=\"\"]\n",
    "\n",
    "def match_llm_output_to_raw_counts(raw, llm):\n",
    "    counts=raw[\"PolicyNo\"].value_counts()\n",
    "    dfs=[]\n",
    "    for pol,n in counts.items():\n",
    "        block=llm[llm[\"PolicyNo\"]==pol]\n",
    "        if len(block)==0: continue\n",
    "        if len(block)>=n:\n",
    "            dfs.append(block.head(n))\n",
    "        else:\n",
    "            dfs.append(pd.concat([block]*((n//len(block))+1)).head(n))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "# DB stubs (you will replace with real code)\n",
    "def get_manhattan_mapping(Load_task_id, company_issuer_id, server, database, log):\n",
    "    return pd.DataFrame(columns=[\"PolicyNo\",\"PlanName\",\"ProductType\"])\n",
    "\n",
    "def stg_plan_mapping_min(df, server, database, log): return len(df)\n",
    "\n",
    "# ---------- Pipeline ----------\n",
    "def run_llm_pipeline(\n",
    "    issuer, paycode, trandate, load_task_id, company_issuer_id,\n",
    "    csv_path, template_dir, output_csv_name,\n",
    "    server_name, database_name,\n",
    "    log=print):\n",
    "\n",
    "    loader=CARRIERS.get(issuer,{}).get(\"loader\",\"csv\")\n",
    "    headers=_fast_read_header(csv_path, loader)\n",
    "    sig=_sig_from_cols(headers)\n",
    "\n",
    "    prompt_path=Path(template_dir)/f\"{issuer}_prompt.txt\"\n",
    "    rules_path=Path(template_dir)/f\"{issuer}_rules.json\"\n",
    "    compiled_path=Path(template_dir)/f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "    if compiled_path.exists():\n",
    "        spec=json.loads(compiled_path.read_text())\n",
    "    else:\n",
    "        raw=llm_generate_rule_spec(headers,prompt_path,rules_path)\n",
    "        raw=canonicalize_spec(raw)\n",
    "        raw=bind_sources_to_headers(headers,raw)\n",
    "        raw=promote_pid_to_ptd(raw)\n",
    "        compiled_path.write_text(json.dumps(raw,indent=2))\n",
    "        spec=raw\n",
    "\n",
    "    usecols=collect_usecols(spec)\n",
    "    df=_read_csv_usecols(csv_path, usecols, loader)\n",
    "\n",
    "    if should_use_ray(len(df)):\n",
    "        out=apply_rules_parallel(df,spec)\n",
    "    else:\n",
    "        out=apply_rules(df,spec)\n",
    "\n",
    "    out[\"TranDate\"]=trandate\n",
    "    out[\"PayCode\"]=paycode\n",
    "    out[\"Issuer\"]=issuer\n",
    "    if \"PlanCode\" not in out: out[\"PlanCode\"]=\"\"\n",
    "\n",
    "    if issuer==\"Manhattan Life\":\n",
    "        log(\"Enriching Manhattan Life…\")\n",
    "        raw_link=extract_manhattan_policy_plan_from_csv(csv_path,log)\n",
    "        map_df=get_manhattan_mapping(load_task_id,company_issuer_id,server_name,database_name,log)\n",
    "\n",
    "        merged=out.merge(raw_link[[\"PolicyNo\",\"PlanCode\"]],on=\"PolicyNo\",how=\"left\")\n",
    "        merged[\"PlanCode\"]=merged[\"PlanCode_x\"].fillna(merged[\"PlanCode_y\"])\n",
    "        merged=merged.drop(columns=[\"PlanCode_x\",\"PlanCode_y\"])\n",
    "        out=match_llm_output_to_raw_counts(raw_link, merged)\n",
    "\n",
    "    out_file=Path(csv_path).parent/f\"{output_csv_name}.csv\"\n",
    "    out.to_csv(out_file,index=False)\n",
    "    log(f\"Done → {out_file}\")\n",
    "    return out_file\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "#   EXAMPLE INPUTS FOR TESTING\n",
    "# ===================================================\n",
    "issuer=\"Manhattan Life\"\n",
    "paycode=\"Default\"\n",
    "trandate=\"2025-11-12\"\n",
    "load_task_id=\"11497\"\n",
    "company_issuer_id=\"3205\"\n",
    "\n",
    "csv_path = r\".\\inbound\\raw.csv\"           # ✔ update\n",
    "template_dir = r\".\\carrier_prompts\"       # ✔ update\n",
    "\n",
    "server_name=\"NGCS\"\n",
    "database_name=\"NGCS\"\n",
    "\n",
    "# ===================================================\n",
    "#   RUN PIPELINE\n",
    "# ===================================================\n",
    "out = run_llm_pipeline(\n",
    "    issuer=issuer,\n",
    "    paycode=paycode,\n",
    "    trandate=trandate,\n",
    "    load_task_id=load_task_id,\n",
    "    company_issuer_id=company_issuer_id,\n",
    "    csv_path=csv_path,\n",
    "    template_dir=template_dir,\n",
    "    output_csv_name=\"manhattan_life_test_output\",\n",
    "    server_name=server_name,\n",
    "    database_name=database_name,\n",
    ")\n",
    "\n",
    "out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
