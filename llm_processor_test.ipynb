{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, re, json, hashlib, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. CONFIG (match what you showed in the screenshot)\n",
    "# ------------------------------------------------------------\n",
    "issuer            = \"Manhattan Life\"\n",
    "paycode           = \"Default\"\n",
    "trandate          = \"2025-11-12\"\n",
    "load_task_id      = \"11497\"\n",
    "company_issuer_id = \"3205\"\n",
    "\n",
    "csv_path     = r\".\\inbound\\raw - 9836c995-9e25-45ae-af39-3b4cc8ac1bbd.csv\"\n",
    "template_dir = r\".\\carrier_prompts\"\n",
    "\n",
    "server_name   = \"QWSD8SQLB401.nguottit.com\"   # or \"NGCS\" etc.\n",
    "database_name = \"NGCS\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. CORE CONSTANTS (only change: PlanCode added to FINAL_COLUMNS)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "CARRIERS = {\n",
    "    \"Molina\": {\"loader\": \"csv\"},\n",
    "    \"Ameritas\": {\"loader\": \"csv\"},\n",
    "    \"Manhattan Life\": {\"loader\": \"two_header\"},\n",
    "}\n",
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNO\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\n",
    "    \"ProductType\",\"PlanName\",\"PlanCode\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"Paysched\",\n",
    "    \"PayCode\",\"WritingAgentID\",\"Premium\",\"CommPrem\",\n",
    "    \"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = [\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. SMALL UTILS (same as processor.py)\n",
    "# ------------------------------------------------------------\n",
    "def _norm_key(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "\n",
    "def _sig_from_cols(cols: List[str]) -> str:\n",
    "    joined = \"||\".join(map(str, cols))\n",
    "    return hashlib.sha1(joined.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def _build_header_index(cols: List[str]) -> Dict[str, str]:\n",
    "    return {_norm_key(h): h for h in cols}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. HEADER PROBE + CSV READER (csv vs two_header)\n",
    "# ------------------------------------------------------------\n",
    "def _fast_read_header(path: str, loader: str) -> List[str]:\n",
    "    if loader == \"csv\":\n",
    "        dfo = pd.read_csv(path, nrows=0, dtype=str)\n",
    "        return list(dfo.columns)\n",
    "\n",
    "    # two_header: first 2 rows\n",
    "    probe = pd.read_csv(path, header=None, nrows=2, dtype=str).fillna(\"\")\n",
    "    top, bottom = probe.iloc[0].tolist(), probe.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    return cols\n",
    "\n",
    "def _read_csv_usecols(path: str,\n",
    "                      usecols: Optional[List[str]],\n",
    "                      loader: str) -> pd.DataFrame:\n",
    "    if loader == \"csv\":\n",
    "        return pd.read_csv(path, dtype=str, usecols=usecols if usecols else None).fillna(\"\")\n",
    "\n",
    "    # two_header\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for a, b in zip(tuple(ff), bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"[\\s+]\", \"_\", name).replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    if usecols:\n",
    "        df = df[[c for c in usecols if c in df.columns]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. RULE SPEC NORMALIZATION + BINDING  (fixes your list .get error)\n",
    "# ------------------------------------------------------------\n",
    "_CANON = {_norm_key(k): k for k in FINAL_COLUMNS + [\"PID\"]}\n",
    "\n",
    "def canonicalize_spec(spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        nk = _norm_key(k)\n",
    "        fixed[_CANON.get(nk, k)] = v\n",
    "    for req in FINAL_COLUMNS:\n",
    "        if req not in fixed and (req != \"PTD\" or \"PID\" not in fixed):\n",
    "            fixed[req] = {\"op\": \"blank\"}\n",
    "    return fixed\n",
    "\n",
    "def normalize_rule_spec(spec_in: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for k, v in spec_in.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            sv = v.strip()\n",
    "            out[k] = {\"op\": \"blank\"} if sv.lower() in (\"blank\", \"tbd\") else {\n",
    "                \"op\": \"const\",\n",
    "                \"value\": sv,\n",
    "            }\n",
    "        else:\n",
    "            out[k] = {\"op\": \"blank\"}\n",
    "    return out\n",
    "\n",
    "def needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\n",
    "        \"money\",\"membercount_from_commission\",\n",
    "    }\n",
    "\n",
    "def bind_sources_to_headers(headers: List[str],\n",
    "                            rule_spec_in: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    norm_map = _build_header_index(headers)\n",
    "    fixed: Dict[str, Any] = {}\n",
    "    spec = normalize_rule_spec(rule_spec_in)   # <-- IMPORTANT: avoids list.get error\n",
    "\n",
    "    for tgt, s in spec.items():\n",
    "        op = str(s.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            fixed[tgt] = {\"op\": \"blank\"}\n",
    "            continue\n",
    "        if not needs_source(op):\n",
    "            fixed[tgt] = s\n",
    "            continue\n",
    "\n",
    "        src = str(s.get(\"source\", \"\")).strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = s\n",
    "            continue\n",
    "\n",
    "        if src in headers:\n",
    "            s[\"source\"] = src\n",
    "        else:\n",
    "            ci = next((h for h in headers if h.lower() == src.lower()), None)\n",
    "            if ci:\n",
    "                s[\"source\"] = ci\n",
    "            else:\n",
    "                nk = _norm_key(src)\n",
    "                if nk in norm_map:\n",
    "                    s[\"source\"] = norm_map[nk]\n",
    "\n",
    "        fixed[tgt] = s\n",
    "\n",
    "    return fixed\n",
    "\n",
    "def promote_pid_to_ptd(spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"PID\" in spec and (\"PTD\" not in spec or\n",
    "                          str(spec[\"PTD\"].get(\"op\", \"\")).lower() in (\"\", \"blank\")):\n",
    "        spec[\"PTD\"] = spec[\"PID\"]\n",
    "    return spec\n",
    "\n",
    "def collect_usecols(bound_spec: Dict[str, Any]) -> List[str]:\n",
    "    cols: set[str] = set()\n",
    "    for _, spec in bound_spec.items():\n",
    "        if isinstance(spec, dict) and needs_source(str(spec.get(\"op\", \"\")).strip()):\n",
    "            src = spec.get(\"source\")\n",
    "            if src:\n",
    "                cols.add(str(src))\n",
    "    return sorted(cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. TRANSFORM FUNCTIONS + apply_rules / Ray\n",
    "# ------------------------------------------------------------\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series):\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    first = s.str.split().str[0].fillna(\"\")\n",
    "    last  = s.str.split().str[-1].fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def apply_rules(df: pd.DataFrame, bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = bound_spec.get(tgt, {\"op\": \"blank\"})\n",
    "        if not isinstance(spec, dict):\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "        op = str(spec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = df.get(s, empty()).astype(str)\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series(\n",
    "                [str(spec.get(\"value\", \"\"))] * len(df),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _to_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _add_one_month_mmddyyyy(df.get(s, empty()))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[0]\n",
    "        elif op == \"name_last_from_full\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = _parse_case_name_first_last(df.get(s, empty()))[1]\n",
    "        elif op == \"money\":\n",
    "            s = spec.get(\"source\")\n",
    "            out[tgt] = df.get(s, empty()).astype(str)\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            s = spec.get(\"source\")\n",
    "            vals = df.get(s, empty()).astype(str)\n",
    "            out[tgt] = pd.Series(\n",
    "                np.where(vals.str.contains(\"-\"), \"-1\", \"1\"),\n",
    "                index=df.index,\n",
    "                dtype=\"string\",\n",
    "            )\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "ENABLE_RAY          = os.getenv(\"ENABLE_RAY\", \"auto\")\n",
    "RAY_PARTITIONS      = int(os.getenv(\"RAY_PARTITIONS\", \"8\"))\n",
    "RAY_MIN_ROWS_TO_USE = int(os.getenv(\"RAY_MIN_ROWS_TO_USE\", \"30000\"))\n",
    "\n",
    "def should_use_ray(n_rows: int) -> bool:\n",
    "    if ENABLE_RAY == \"on\":\n",
    "        return True\n",
    "    if ENABLE_RAY == \"off\":\n",
    "        return False\n",
    "    return n_rows >= RAY_MIN_ROWS_TO_USE\n",
    "\n",
    "def apply_rules_parallel(df: pd.DataFrame,\n",
    "                         bound_spec: Dict[str, Any]) -> pd.DataFrame:\n",
    "    import ray\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(ignore_reinit_error=True, include_dashboard=False, log_to_driver=False)\n",
    "    spec_ref = ray.put(bound_spec)\n",
    "\n",
    "    @ray.remote\n",
    "    def _worker(chunk: pd.DataFrame, spec_ref):\n",
    "        return apply_rules(chunk, ray.get(spec_ref))\n",
    "\n",
    "    parts = np.array_split(df, max(1, RAY_PARTITIONS))\n",
    "    futures = [_worker.remote(part, spec_ref) for part in parts]\n",
    "    outs = ray.get(futures)\n",
    "    return pd.concat(outs, ignore_index=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. MINI-PIPELINE (everything up to BEFORE Manhattan enrichment)\n",
    "# ------------------------------------------------------------\n",
    "loader      = CARRIERS.get(issuer, {}).get(\"loader\", \"csv\")\n",
    "headers     = _fast_read_header(csv_path, loader)\n",
    "sig         = _sig_from_cols(headers)\n",
    "\n",
    "prompt_path   = Path(template_dir) / f\"{issuer}_prompt.txt\"\n",
    "rules_path    = Path(template_dir) / f\"{issuer}_rules.json\"\n",
    "compiled_path = Path(template_dir) / f\"{issuer}_compiled_rules_{sig}.json\"\n",
    "\n",
    "# --- load or build rules ---\n",
    "if compiled_path.exists():\n",
    "    bound_spec = json.loads(compiled_path.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    # === SAME behavior as llm_processor_dbconn.py ===\n",
    "    raw_spec = llm_generate_rule_spec(headers, prompt_path, rules_path)\n",
    "    raw_spec = canonicalize_spec(raw_spec)\n",
    "    raw_spec = bind_sources_to_headers(headers, raw_spec)\n",
    "    raw_spec = promote_pid_to_ptd(raw_spec)\n",
    "\n",
    "    compiled_path.write_text(\n",
    "        json.dumps(raw_spec, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    bound_spec = raw_spec\n",
    "\n",
    "# Make sure spec structure is normalized like in processor.py\n",
    "bound_spec = canonicalize_spec(bound_spec)\n",
    "bound_spec = bind_sources_to_headers(headers, bound_spec)\n",
    "bound_spec = promote_pid_to_ptd(bound_spec)\n",
    "\n",
    "usecols = collect_usecols(bound_spec)\n",
    "df = _read_csv_usecols(csv_path, usecols if usecols else None, loader)\n",
    "print(f\"Rows loaded: {len(df)}, usecols: {usecols}\")\n",
    "\n",
    "if should_use_ray(len(df)):\n",
    "    out_df = apply_rules_parallel(df, bound_spec)\n",
    "else:\n",
    "    out_df = apply_rules(df, bound_spec)\n",
    "\n",
    "# ---- ADD CONSTANTS + PLANCODE BEFORE MANHATTAN ENRICHMENT ----\n",
    "out_df[\"TranDate\"] = trandate\n",
    "out_df[\"PayCode\"]  = paycode\n",
    "out_df[\"Issuer\"]   = issuer\n",
    "out_df[\"ProductType\"] = \"\"\n",
    "out_df[\"PlanName\"]    = \"\"\n",
    "if \"PlanCode\" not in out_df.columns:\n",
    "    out_df[\"PlanCode\"] = \"\"      # <-- PlanCode guaranteed BEFORE #6\n",
    "out_df[\"Note\"] = \"\"\n",
    "\n",
    "display(out_df.head())\n",
    "display(out_df[[\"PolicyNO\",\"PlanCode\"]].head(20))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
