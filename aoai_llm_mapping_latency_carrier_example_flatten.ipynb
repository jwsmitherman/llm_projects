{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769a266d",
   "metadata": {},
   "source": [
    "\n",
    "# Azure OpenAI Latency Test — Carrier (Two-Row Header Flattening)\n",
    "\n",
    "This notebook measures **LLM response time** for generating a **field-mapping rule-spec JSON** using your uploaded Manhattan Life CSV.\n",
    "\n",
    "**Key features:**\n",
    "- Uses your two-row header flattener to normalize headers from `/mnt/data/manhattan_life_raw_data.csv`.\n",
    "- Sends **flattened headers** + your **carrier prompt** to Azure OpenAI once per trial.\n",
    "- Reports mean/median/min/max/p95 latency and prints the last JSON rule-spec (truncated).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669a3ee",
   "metadata": {},
   "source": [
    "## 0) Environment — Azure OpenAI settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Set these to your actual values (or set them in your environment before running)\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"<YOUR_API_KEY>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://<your-resource>.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-4o-mini\")  # your chat deployment name\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8ba37",
   "metadata": {},
   "source": [
    "## 1) Utilities — Two-Row Header Flattener (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "## Load and flatten multi-row header\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str)\n",
    "    tmp = tmp.fillna(\"\")\n",
    "\n",
    "    # First two rows are headers\n",
    "    top = tmp.iloc[0].tolist()\n",
    "    bottom = tmp.iloc[1].tolist()\n",
    "\n",
    "    # Forward-fill across top header row\n",
    "    ff = []\n",
    "    last = \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    # Build merged column names\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "\n",
    "        # Normalize names (remove spaces, slashes, periods)\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    # Drop the two header rows, assign new columns\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "\n",
    "    # Drop empty columns\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31594e",
   "metadata": {},
   "source": [
    "## 2) Load & Inspect Flattened Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path(\"/mnt/data/manhattan_life_raw_data.csv\")\n",
    "if not RAW_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {RAW_PATH}\")\n",
    "\n",
    "df_flat = flatten_two_header_csv(RAW_PATH.as_posix())\n",
    "CSV_HEADERS = list(df_flat.columns)\n",
    "\n",
    "print(\"Detected flattened headers (count =\", len(CSV_HEADERS), \"):\\n\", CSV_HEADERS[:25], \"...\")\n",
    "print(\"Data preview:\")\n",
    "df_flat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938eff8",
   "metadata": {},
   "source": [
    "## 3) Required destination fields & (paste) Manhattan Life prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "# ⬇️ Paste your real Manhattan Life prompt text here\n",
    "RULES_TEXT = \"\"\"\n",
    "You are a data transformation agent...\n",
    "(Replace this with the exact text of carrier_prompts/manhattan_life_prompt.txt)\n",
    "If a rule is unclear or TBD, return blank for that field.\n",
    "Use 'PID' if the rule refers to PID but output needs PTD (executor will map PID→PTD).\n",
    "Return STRICT JSON only. No prose.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6505245",
   "metadata": {},
   "source": [
    "## 4) AzureChatOpenAI setup and latency benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, statistics, time\n",
    "\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import AzureChatOpenAI  # fallback\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Output JSON ONLY, no prose.\n",
    "Return a mapping where keys are required output fields and values are objects:\n",
    "{ \"op\": <one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,money,membercount_from_commission,blank]>, \n",
    "  \"source\": <column name when applicable>, \n",
    "  \"value\": <for const> }.\n",
    "If a rule says 'TBD' or 'blank' or is unclear, use {\"op\":\"blank\"}.\n",
    "Never invent columns. Use exact source header strings when copying.\n",
    "Use 'PID' if rules refer to PID but output needs PTD.\n",
    "\"\"\"\n",
    "\n",
    "def build_llm(timeout: int = 20, temperature: float = 1.0) -> AzureChatOpenAI:\n",
    "    # Note: some Azure deployments lock temperature to 1.0; pass 1.0 to avoid 400 errors.\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "    if not (api_key and endpoint and deployment):\n",
    "        raise RuntimeError(\"Missing AZURE_OPENAI_* env vars\")\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "        max_retries=0  # raw latency\n",
    "    )\n",
    "\n",
    "def time_rule_spec_build(headers, rules_text, trials: int = 5, timeout: int = 20, temperature: float = 1.0):\n",
    "    llm = build_llm(timeout=timeout, temperature=temperature)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": json.dumps({\n",
    "            \"RequiredFields\": FINAL_COLUMNS,\n",
    "            \"RawHeaders\": headers,\n",
    "            \"RulesNarrative\": rules_text,\n",
    "            \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "        }, ensure_ascii=False)}\n",
    "    ]\n",
    "    latencies, outputs = [], []\n",
    "\n",
    "    # Warmup (not counted)\n",
    "    try:\n",
    "        _ = llm.invoke(messages)\n",
    "    except Exception as e:\n",
    "        print(\"Warmup failed:\", e)\n",
    "\n",
    "    for i in range(trials):\n",
    "        t0 = time.perf_counter()\n",
    "        resp = llm.invoke(messages)\n",
    "        t1 = time.perf_counter()\n",
    "        latencies.append(t1 - t0)\n",
    "        outputs.append(resp.content if hasattr(resp, \"content\") else str(resp))\n",
    "\n",
    "    lat_sorted = sorted(latencies)\n",
    "    p95 = lat_sorted[int(0.95*(len(lat_sorted)-1))] if latencies else None\n",
    "    return {\n",
    "        \"trials\": trials,\n",
    "        \"latencies_s\": latencies,\n",
    "        \"mean_s\": statistics.fmean(latencies) if latencies else None,\n",
    "        \"median_s\": statistics.median(latencies) if latencies else None,\n",
    "        \"min_s\": min(latencies) if latencies else None,\n",
    "        \"max_s\": max(latencies) if latencies else None,\n",
    "        \"p95_s\": p95,\n",
    "        \"last_output\": outputs[-1] if outputs else \"\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c686c6c",
   "metadata": {},
   "source": [
    "## 5) Run the latency benchmark (using flattened headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = time_rule_spec_build(headers=CSV_HEADERS, rules_text=RULES_TEXT, trials=5, timeout=20, temperature=1.0)\n",
    "print(\"Latency (seconds):\")\n",
    "print({k: v for k, v in results.items() if k not in (\"latencies_s\",\"last_output\")})\n",
    "print(\"\\nRaw latencies per trial (s):\", results[\"latencies_s\"])\n",
    "print(\"\\nLast JSON output (truncated to 1200 chars):\\n\")\n",
    "print(results[\"last_output\"][:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def _needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\n",
    "        \"money\",\"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def audit_rule_spec(df: pd.DataFrame, rule_spec: dict, topk: int = 3):\n",
    "    headers = list(df.columns)\n",
    "    headers_norm = {re.sub(r'[^a-z0-9]', '', h.lower()): h for h in headers}\n",
    "    missing = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        if tgt == \"PID\":  # will map to PTD\n",
    "            pass\n",
    "        op = spec.get(\"op\",\"\")\n",
    "        if not _needs_source(op):\n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            missing.append((tgt, op, src, [], []))\n",
    "            continue\n",
    "        # exact (case-insensitive) check\n",
    "        if src in headers or src.lower() in [h.lower() for h in headers]:\n",
    "            continue\n",
    "        # normalized check\n",
    "        src_norm = re.sub(r'[^a-z0-9]', '', src.lower())\n",
    "        if src_norm in headers_norm:\n",
    "            continue\n",
    "        # fuzzy suggestions\n",
    "        candidates = get_close_matches(src, headers, n=topk) or []\n",
    "        candidates2 = get_close_matches(src_norm, [re.sub(r'[^a-z0-9]','',h.lower()) for h in headers], n=topk)\n",
    "        cand2_orig = []\n",
    "        for c2 in candidates2:\n",
    "            for h in headers:\n",
    "                if re.sub(r'[^a-z0-9]','',h.lower()) == c2:\n",
    "                    cand2_orig.append(h)\n",
    "        missing.append((tgt, op, src, candidates, cand2_orig))\n",
    "    return missing\n",
    "\n",
    "# After you have df_flat and rule_spec:\n",
    "missing = audit_rule_spec(df_flat, rule_spec)\n",
    "for tgt, op, src, c1, c2 in missing:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) source='{src}'\")\n",
    "    if c1: print(\"  close (raw):\", c1)\n",
    "    if c2: print(\"  close (norm):\", c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ffde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "def _build_header_index(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a robust index: normalized header -> original header.\n",
    "    Normalization: lowercase, remove non-alphanumerics.\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for h in df.columns:\n",
    "        key = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[key] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: Dict[str, str], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Case/punctuation/underscore-insensitive lookup.\n",
    "    Tries raw exact, case-insensitive, normalized match.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "    # raw exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    # case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    # normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    # give up\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d32fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Load rules from JSON (not inline), then compile\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "RULES_FILE = Path(\"carrier_prompts/manhattan_life_rules.json\")\n",
    "RULES_TEXT = RULES_FILE.read_text(encoding=\"utf-8\")  # keep the rules outside the code\n",
    "\n",
    "rule_spec = derive_rule_spec(\n",
    "    required_fields=FINAL_COLUMNS,   # actually use the arg\n",
    "    headers=list(df_flat.columns),\n",
    "    rules_text=RULES_TEXT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f90a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_rule_spec(required_fields, headers, rules_text) -> dict:\n",
    "    llm = build_llm(temperature=1.0)\n",
    "    payload = {\n",
    "        \"RequiredFields\": required_fields,   # <-- not FINAL_COLUMNS\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": rules_text,\n",
    "        \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "    ])\n",
    "    return json.loads(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Make header lookup robust (spaces vs underscores, case, punctuation)\n",
    "import re\n",
    "\n",
    "def _build_header_index(df):\n",
    "    # normalized header -> original header\n",
    "    idx = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df, header_index, name: str):\n",
    "    if not name:\n",
    "        return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    # exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    # case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    # normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    # not found\n",
    "    return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ad433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules_vectorized(df, rule_spec):\n",
    "    header_index = _build_header_index(df)\n",
    "    ...\n",
    "    # replace previous calls with:\n",
    "    src = _get_col(df, header_index, spec.get(\"source\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b281fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Audit mismatches before transforming\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def _needs_src(op:str) -> bool:\n",
    "    return op in {\"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "                  \"name_first_from_full\",\"name_last_from_full\",\n",
    "                  \"money\",\"membercount_from_commission\"}\n",
    "\n",
    "def audit_rule_spec(df, rule_spec, topk=3):\n",
    "    headers = list(df.columns)\n",
    "    norm = {re.sub(r'[^a-z0-9]','',h.lower()):h for h in headers}\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        op = spec.get(\"op\",\"\")\n",
    "        if not _needs_src(op): \n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            print(f\"[MISSING] {tgt} ({op}) source=''\")\n",
    "            continue\n",
    "        if src in headers or any(h.lower()==src.lower() for h in headers):\n",
    "            continue\n",
    "        key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "        if key in norm:\n",
    "            continue\n",
    "        c1 = get_close_matches(src, headers, n=topk)\n",
    "        c2 = get_close_matches(key, list(norm.keys()), n=topk)\n",
    "        c2 = [norm[k] for k in c2]\n",
    "        print(f\"[MISSING] {tgt} <- ({op}) source='{src}'\")\n",
    "        if c1: print(\"  close (raw):\", c1)\n",
    "        if c2: print(\"  close (norm):\", c2)\n",
    "\n",
    "audit_rule_spec(df_flat, rule_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Quick end-to-end (after you fix the rules file)\n",
    "# 1) Flatten\n",
    "df_flat = flatten_two_header_csv(\"/mnt/data/manhattan_life_raw_data.csv\")\n",
    "\n",
    "# 2) Build spec once\n",
    "RULES_TEXT = Path(\"carrier_prompts/manhattan_life_rules.json\").read_text(encoding=\"utf-8\")\n",
    "rule_spec = derive_rule_spec(FINAL_COLUMNS, list(df_flat.columns), RULES_TEXT)\n",
    "\n",
    "# 3) Audit (should print nothing if all aligned)\n",
    "audit_rule_spec(df_flat, rule_spec)\n",
    "\n",
    "# 4) Transform\n",
    "out_df = apply_rules_vectorized(df_flat, rule_spec)\n",
    "\n",
    "# 5) Save\n",
    "out_df.to_csv(\"/mnt/data/outbound/manhattan_life_standard.csv\", index=False)\n",
    "Path(\"/mnt/data/outbound/manhattan_life_standard.json\").write_text(\n",
    "    out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44332bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ---- Standard output schema (order preserved) ----\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "# ---------- Robust header lookup ----------\n",
    "def _build_header_index(df: pd.DataFrame) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a normalized header -> original header index.\n",
    "    Normalization: lowercase, strip all non [a-z0-9].\n",
    "    \"\"\"\n",
    "    idx: dict[str, str] = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: dict[str, str], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Case/underscore/punctuation insensitive column fetch.\n",
    "    Returns empty-string Series if not found.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    # 1) exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "\n",
    "    # 2) case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "\n",
    "    # 3) normalized\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "\n",
    "    # not found\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# ---------- Vectorized helpers ----------\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2:\n",
    "            return \" \".join(parts[1:] + parts[:1])  # move leading token to end\n",
    "        return name\n",
    "\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return pd.Series(num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\"), index=s.index, dtype=\"string\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# ---------- Main compiler/executor ----------\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\n",
    "    \"money\",\"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply LLM-compiled rule_spec to df and return a DataFrame with FINAL_COLUMNS.\n",
    "    Supported spec entry for each target:\n",
    "      {\"op\":\"copy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"const\",\"value\":\"Active\"}\n",
    "      {\"op\":\"date_mmddyyyy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"date_plus_1m_mmddyyyy\",\"source\":\"Col\"}\n",
    "      {\"op\":\"name_first_from_full\",\"source\":\"Col\"}\n",
    "      {\"op\":\"name_last_from_full\",\"source\":\"Col\"}\n",
    "      {\"op\":\"money\",\"source\":\"Col\"}\n",
    "      {\"op\":\"membercount_from_commission\",\"source\":\"Col\"}  # \"1\" unless <0 → \"-1\"; blank→\"1\"\n",
    "      {\"op\":\"blank\"}\n",
    "    Special: if spec contains \"PID\", it is written to output \"PTD\".\n",
    "    \"\"\"\n",
    "    header_index = _build_header_index(df)\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    out: dict[str, pd.Series] = {}\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        # Allow alias: PID (rule) -> PTD (output)\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not spec or not isinstance(spec, dict):\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        op = str(spec.get(\"op\", \"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty()\n",
    "            continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            out[tgt] = _get_col(df, header_index, spec.get(\"source\", \"\"))\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series([str(spec.get(\"value\", \"\"))] * len(df), index=df.index, dtype=\"string\")\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            first, _ = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = first\n",
    "        elif op == \"name_last_from_full\":\n",
    "            _, last = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = last\n",
    "        elif op == \"money\":\n",
    "            out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\", \"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif op == \"blank\":\n",
    "            out[tgt] = empty()\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    # Ensure order + types\n",
    "    out_df = pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813d4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6143a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87bfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & constants ===================================================\n",
    "from __future__ import annotations\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "    \"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "\n",
    "# === 1) CSV flattener (two-row header) =======================================\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str)\n",
    "    tmp = tmp.fillna(\"\")\n",
    "\n",
    "    top = tmp.iloc[0].tolist()\n",
    "    bottom = tmp.iloc[1].tolist()\n",
    "\n",
    "    # forward-fill top\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    # combine + normalize\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if not a and not b:\n",
    "            name = \"unnamed\"\n",
    "        elif not a:\n",
    "            name = b\n",
    "        elif not b:\n",
    "            name = a\n",
    "        else:\n",
    "            name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    # drop fully-empty columns\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# === 2) Robust header lookup ==================================================\n",
    "def _build_header_index(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    # normalized header -> original header\n",
    "    idx: Dict[str, str] = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r\"[^a-z0-9]\", \"\", str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: Dict[str, str], name: str) -> pd.Series:\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "    # exact\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    # case-insensitive exact\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    # normalized\n",
    "    key = re.sub(r\"[^a-z0-9]\", \"\", name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    # not found\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# === 3) Vectorized helpers ====================================================\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2:\n",
    "            return \" \".join(parts[1:] + parts[:1])\n",
    "        return name\n",
    "\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return pd.Series(num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\"), index=s.index, dtype=\"string\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[,$()]\", \"\", regex=True).str.strip()\n",
    "    x = np.where(neg_paren, \"-\" + x, x)\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# === 4) Auto-bind rule sources to actual headers ==============================\n",
    "def needs_source(spec: Dict[str, Any]) -> bool:\n",
    "    return spec.get(\"op\") in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\n",
    "        \"money\",\"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def bind_rule_sources_to_headers(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For any rule that references a non-existent 'source', try to resolve it\n",
    "    to a real header (exact, case-insensitive, normalized, or fuzzy close match).\n",
    "    Returns a **new** rule_spec with corrected 'source' values.\n",
    "    \"\"\"\n",
    "    headers = list(df.columns)\n",
    "    norm_map = {re.sub(r'[^a-z0-9]', '', h.lower()): h for h in headers}\n",
    "    fixed = {}\n",
    "\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        spec = dict(spec)  # copy\n",
    "        if not needs_source(spec):\n",
    "            fixed[tgt] = spec\n",
    "            continue\n",
    "\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = spec\n",
    "            continue\n",
    "\n",
    "        # 1) exact / case-insensitive\n",
    "        cand = None\n",
    "        if src in headers:\n",
    "            cand = src\n",
    "        else:\n",
    "            for h in headers:\n",
    "                if h.lower() == src.lower():\n",
    "                    cand = h\n",
    "                    break\n",
    "\n",
    "        # 2) normalized\n",
    "        if cand is None:\n",
    "            key = re.sub(r'[^a-z0-9]', '', src.lower())\n",
    "            if key in norm_map:\n",
    "                cand = norm_map[key]\n",
    "\n",
    "        # 3) fuzzy (raw then normalized)\n",
    "        if cand is None:\n",
    "            c1 = get_close_matches(src, headers, n=1)\n",
    "            if c1:\n",
    "                cand = c1[0]\n",
    "            else:\n",
    "                key = re.sub(r'[^a-z0-9]', '', src.lower())\n",
    "                c2 = get_close_matches(key, list(norm_map.keys()), n=1)\n",
    "                if c2:\n",
    "                    cand = norm_map[c2[0]]\n",
    "\n",
    "        if cand:\n",
    "            spec[\"source\"] = cand  # rewrite to the actual header\n",
    "        fixed[tgt] = spec\n",
    "\n",
    "    return fixed\n",
    "\n",
    "def audit_missing_sources(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]):\n",
    "    headers = list(df.columns)\n",
    "    norm = {re.sub(r'[^a-z0-9]','',h.lower()):h for h in headers}\n",
    "    misses = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        if not needs_source(spec): \n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src: \n",
    "            misses.append((tgt, spec.get(\"op\"), src, [])); \n",
    "            continue\n",
    "        ok = (src in headers) or any(h.lower()==src.lower() for h in headers) \\\n",
    "             or (re.sub(r'[^a-z0-9]','',src.lower()) in norm)\n",
    "        if not ok:\n",
    "            sugg = get_close_matches(src, headers, n=3)\n",
    "            if not sugg:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                sugg = [norm[k] for k in get_close_matches(key, list(norm.keys()), n=3)]\n",
    "            misses.append((tgt, spec.get(\"op\"), src, sugg))\n",
    "    return misses\n",
    "\n",
    "# === 5) Rule executor =========================================================\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies rule_spec to df and returns a DataFrame with FINAL_COLUMNS.\n",
    "    Supports PID in spec -> PTD in output.\n",
    "    \"\"\"\n",
    "    header_index = _build_header_index(df)\n",
    "\n",
    "    def empty() -> pd.Series:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not isinstance(spec, dict):\n",
    "            out[tgt] = empty(); continue\n",
    "\n",
    "        op = str(spec.get(\"op\",\"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            out[tgt] = empty(); continue\n",
    "\n",
    "        if op == \"copy\":\n",
    "            out[tgt] = _get_col(df, header_index, spec.get(\"source\",\"\"))\n",
    "        elif op == \"const\":\n",
    "            out[tgt] = pd.Series([str(spec.get(\"value\",\"\"))]*len(df), index=df.index, dtype=\"string\")\n",
    "        elif op == \"date_mmddyyyy\":\n",
    "            out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\":\n",
    "            out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"name_first_from_full\":\n",
    "            first, _ = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = first\n",
    "        elif op == \"name_last_from_full\":\n",
    "            _, last = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = last\n",
    "        elif op == \"money\":\n",
    "            out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif op == \"blank\":\n",
    "            out[tgt] = empty()\n",
    "        else:\n",
    "            out[tgt] = empty()\n",
    "\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# === 6) Example: end-to-end for your file ====================================\n",
    "# 6a) Load + flatten\n",
    "RAW = \"/mnt/data/manhattan_life_raw_data.csv\"      # <-- your uploaded file\n",
    "df_flat = flatten_two_header_csv(RAW)\n",
    "print(\"Flattened headers:\", list(df_flat.columns)[:40], \"...\")\n",
    "\n",
    "# 6b) Your Manhattan Life RULES_TEXT (natural-language mapping) -------------\n",
    "# (If you already have an LLM-compiled spec with 'op/source/value', load it instead.)\n",
    "RULES_TEXT = \"\"\"\n",
    "{\n",
    "  \"PolicyNo\": \"Policy\",\n",
    "  \"PHFirst\": \"Owner Name, derive first name before last (supports 'LAST, FIRST')\",\n",
    "  \"PHLast\": \"Owner Name, derive last name (supports 'LAST, FIRST')\",\n",
    "  \"Status\": \"Default to Active\",\n",
    "  \"Issuer\": \"Manhattan Life\",\n",
    "  \"State\": \"Issue State\",\n",
    "  \"ProductType\": \"Plan Code\",\n",
    "  \"PlanName\": \"Plan Description\",\n",
    "  \"SubmittedDate\": \"Payment Date\",\n",
    "  \"EffectiveDate\": \"Payment Date\",\n",
    "  \"TermDate\": \"blank\",\n",
    "  \"PaySched\": \"Default to Monthly\",\n",
    "  \"PayCode\": \"Default to Default\",\n",
    "  \"WritingAgentID\": \"Writing Agent\",\n",
    "  \"Premium\": \"Commission\",\n",
    "  \"CommPrem\": \"Commission\",\n",
    "  \"TranDate\": 'Payment Date',\n",
    "  \"CommReceived\": \"Commission\",\n",
    "  \"PID\": \"Paid To Date (plus 1 month)\",\n",
    "  \"NoPayMon\": \"Default to -1\",\n",
    "  \"Membercount\": \"Default to 1 unless Commission is negative then -1\",\n",
    "  \"Note\": \"blank\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 6c) If you already have an LLM-compiled spec, load it here instead of compiling.\n",
    "# For illustration, we make a minimal compiled spec by hand (you likely have this from your LLM step):\n",
    "compiled_rule_spec = {\n",
    "    \"PolicyNo\": {\"op\":\"copy\",\"source\":\"Policy\"},\n",
    "    \"PHFirst\": {\"op\":\"name_first_from_full\",\"source\":\"Owner Name\"},\n",
    "    \"PHLast\":  {\"op\":\"name_last_from_full\",\"source\":\"Owner Name\"},\n",
    "    \"Status\":  {\"op\":\"const\",\"value\":\"Active\"},\n",
    "    \"Issuer\":  {\"op\":\"const\",\"value\":\"Manhattan Life\"},\n",
    "    \"State\":   {\"op\":\"copy\",\"source\":\"Issue State\"},\n",
    "    \"ProductType\":{\"op\":\"copy\",\"source\":\"Plan Code\"},\n",
    "    \"PlanName\":   {\"op\":\"copy\",\"source\":\"Plan Description\"},\n",
    "    \"SubmittedDate\":{\"op\":\"date_mmddyyyy\",\"source\":\"Payment Date\"},\n",
    "    \"EffectiveDate\":{\"op\":\"date_mmddyyyy\",\"source\":\"Payment Date\"},\n",
    "    \"TermDate\":{\"op\":\"blank\"},\n",
    "    \"PaySched\":{\"op\":\"const\",\"value\":\"Monthly\"},\n",
    "    \"PayCode\":{\"op\":\"const\",\"value\":\"Default\"},\n",
    "    \"WritingAgentID\":{\"op\":\"copy\",\"source\":\"Writing Agent\"},\n",
    "    \"Premium\":{\"op\":\"money\",\"source\":\"Commission\"},\n",
    "    \"CommPrem\":{\"op\":\"money\",\"source\":\"Commission\"},\n",
    "    \"TranDate\":{\"op\":\"date_mmddyyyy\",\"source\":\"Payment Date\"},\n",
    "    \"CommReceived\":{\"op\":\"money\",\"source\":\"Commission\"},\n",
    "    \"PID\":{\"op\":\"date_plus_1m_mmddyyyy\",\"source\":\"Paid To Date\"},\n",
    "    \"NoPayMon\":{\"op\":\"const\",\"value\":\"-1\"},\n",
    "    \"Membercount\":{\"op\":\"membercount_from_commission\",\"source\":\"Commission\"}\n",
    "}\n",
    "\n",
    "# 6d) Auto-bind the sources to the actual flattened headers\n",
    "#     (handles underscores, extra suffixes like \"..._Date\", etc.)\n",
    "bound_spec = bind_rule_sources_to_headers(df_flat, compiled_rule_spec)\n",
    "\n",
    "# Audit what is still missing (should be empty or very small)\n",
    "misses = audit_missing_sources(df_flat, bound_spec)\n",
    "for tgt, op, src, sugg in misses:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) '{src}'  suggestions:\", sugg)\n",
    "\n",
    "# 6e) Apply vectorized rules\n",
    "out_df = apply_rules_vectorized(df_flat, bound_spec)\n",
    "\n",
    "# 6f) Preview + save\n",
    "print(out_df.head().T)\n",
    "OUT_DIR = Path(\"/mnt/data/outbound\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "out_df.to_csv(OUT_DIR / \"manhattan_life_standard.csv\", index=False)\n",
    "(Path(OUT_DIR / \"manhattan_life_standard.json\")\n",
    "    .write_text(out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\"))\n",
    "print(\"Wrote:\", (OUT_DIR / \"manhattan_life_standard.csv\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89485a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad92ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c409a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports & constants\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# STANDARD OUTPUT SCHEMA\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "    \"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 1) Azure OpenAI (LLM)\n",
    "# =========================\n",
    "# Set these once (env or here)\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"<YOUR_API_KEY>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://<your-resource>.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-4o-mini\")  # your chat deployment\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    # older langchain\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "def build_llm(timeout: int = 20, temperature: float = 1.0) -> AzureChatOpenAI:\n",
    "    # Some Azure deployments lock temperature=1. Use 1.0 unless you know yours supports 0.0\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "    if not (api_key and endpoint and deployment):\n",
    "        raise RuntimeError(\"Missing AZURE_OPENAI_* env vars\")\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "        max_retries=0\n",
    "    )\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Output JSON ONLY, no prose.\n",
    "Return a mapping where keys are required output fields and values are objects:\n",
    "{ \"op\": <one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,money,membercount_from_commission,blank]>, \n",
    "  \"source\": <column name when applicable>, \n",
    "  \"value\": <for const> }.\n",
    "If a rule says 'TBD' or 'blank' or is unclear, use {\"op\":\"blank\"}.\n",
    "Never invent columns. Use exact source header strings when copying.\n",
    "Use 'PID' if rules refer to PID but output needs PTD.\n",
    "\"\"\"\n",
    "\n",
    "def derive_rule_spec(llm: AzureChatOpenAI, required_fields, headers, rules_text: str) -> dict:\n",
    "    \"\"\"Call the LLM ONCE to compile natural-language RULES_TEXT into an operational rule_spec.\"\"\"\n",
    "    payload = {\n",
    "        \"RequiredFields\": required_fields,\n",
    "        \"RawHeaders\": headers,\n",
    "        \"RulesNarrative\": rules_text,\n",
    "        \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "    }\n",
    "    resp = llm.invoke([\n",
    "        {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "    ])\n",
    "    # Raise helpful errors if output isn't JSON\n",
    "    try:\n",
    "        return json.loads(resp.content if hasattr(resp, \"content\") else str(resp))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"LLM did not return valid JSON. Raw: {resp}\") from e\n",
    "\n",
    "# =========================\n",
    "# 2) CSV flattener (two-row header)\n",
    "# =========================\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str)\n",
    "    tmp = tmp.fillna(\"\")\n",
    "    top = tmp.iloc[0].tolist()\n",
    "    bottom = tmp.iloc[1].tolist()\n",
    "    # forward-fill top row\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "    # combine + normalize\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a = str(a).strip()\n",
    "        b = str(b).strip()\n",
    "        if not a and not b: name = \"unnamed\"\n",
    "        elif not a:         name = b\n",
    "        elif not b:         name = a\n",
    "        else:               name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# =========================\n",
    "# 3) Robust header lookup\n",
    "# =========================\n",
    "def _build_header_index(df: pd.DataFrame) -> dict[str, str]:\n",
    "    idx: dict[str, str] = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: dict[str, str], name: str) -> pd.Series:\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# 4) Vectorized helpers\n",
    "# =========================\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name: return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2: return \" \".join(parts[1:] + parts[:1])\n",
    "        return name\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    return num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# 5) Auto-bind LLM sources → real headers (fix mismatches)\n",
    "# =========================\n",
    "def _needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "        \"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def bind_rule_sources_to_headers(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    headers = list(df.columns)\n",
    "    norm_map = {re.sub(r'[^a-z0-9]','',h.lower()): h for h in headers}\n",
    "    fixed: Dict[str, Dict[str, Any]] = {}\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        spec = dict(spec)\n",
    "        op = str(spec.get(\"op\",\"\"))\n",
    "        if not _needs_source(op):\n",
    "            fixed[tgt] = spec; continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = spec; continue\n",
    "        # exact (raw / case-insensitive)\n",
    "        cand = next((h for h in headers if h == src or h.lower() == src.lower()), None)\n",
    "        # normalized\n",
    "        if cand is None:\n",
    "            key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "            cand = norm_map.get(key)\n",
    "        # fuzzy\n",
    "        if cand is None:\n",
    "            raw = get_close_matches(src, headers, n=1)\n",
    "            if raw: cand = raw[0]\n",
    "            else:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                norm = get_close_matches(key, list(norm_map.keys()), n=1)\n",
    "                if norm: cand = norm_map[norm[0]]\n",
    "        if cand: spec[\"source\"] = cand\n",
    "        fixed[tgt] = spec\n",
    "    return fixed\n",
    "\n",
    "def audit_missing_sources(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]):\n",
    "    headers = list(df.columns)\n",
    "    norm_map = {re.sub(r'[^a-z0-9]','',h.lower()): h for h in headers}\n",
    "    misses = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        op = str(spec.get(\"op\",\"\"))\n",
    "        if not _needs_source(op): \n",
    "            continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            misses.append((tgt, op, src, [])); continue\n",
    "        ok = src in headers or any(h.lower()==src.lower() for h in headers) or (re.sub(r'[^a-z0-9]','',src.lower()) in norm_map)\n",
    "        if not ok:\n",
    "            sugg = get_close_matches(src, headers, n=3)\n",
    "            if not sugg:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                sugg = [norm_map[k] for k in get_close_matches(key, list(norm_map.keys()), n=3)]\n",
    "            misses.append((tgt, op, src, sugg))\n",
    "    return misses\n",
    "\n",
    "# =========================\n",
    "# 6) Executor\n",
    "# =========================\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "    header_index = _build_header_index(df)\n",
    "    def empty() -> pd.Series: return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not isinstance(spec, dict): out[tgt] = empty(); continue\n",
    "        op = str(spec.get(\"op\",\"\"))\n",
    "        if op not in ALLOWED_OPS: out[tgt] = empty(); continue\n",
    "        if     op == \"copy\":                     out[tgt] = _get_col(df, header_index, spec.get(\"source\",\"\"))\n",
    "        elif   op == \"const\":                    out[tgt] = pd.Series([str(spec.get(\"value\",\"\"))]*len(df), index=df.index, dtype=\"string\")\n",
    "        elif   op == \"date_mmddyyyy\":            out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif   op == \"date_plus_1m_mmddyyyy\":    out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif   op == \"name_first_from_full\":     out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[0]\n",
    "        elif   op == \"name_last_from_full\":      out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[1]\n",
    "        elif   op == \"money\":                    out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif   op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif   op == \"blank\":                    out[tgt] = empty()\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# =========================\n",
    "# 7) RUN: flatten → LLM compile → bind → apply → save\n",
    "# =========================\n",
    "RAW = \"/mnt/data/manhattan_life_raw_data.csv\"  # your uploaded file\n",
    "df_flat = flatten_two_header_csv(RAW)\n",
    "print(\"Flattened headers (first 20):\", list(df_flat.columns)[:20])\n",
    "\n",
    "# Your Manhattan Life rules (natural language JSON). You can also do Path(...).read_text()\n",
    "RULES_TEXT = \"\"\"\n",
    "{\n",
    "  \"PolicyNo\": \"Policy\",\n",
    "  \"PHFirst\": \"Owner Name, derive first name before last (supports 'LAST, FIRST')\",\n",
    "  \"PHLast\": \"Owner Name, derive last name (supports 'LAST, FIRST')\",\n",
    "  \"Status\": \"Default to Active\",\n",
    "  \"Issuer\": \"Manhattan Life\",\n",
    "  \"State\": \"Issue State\",\n",
    "  \"ProductType\": \"Plan Code\",\n",
    "  \"PlanName\": \"Plan Description\",\n",
    "  \"SubmittedDate\": \"Payment Date\",\n",
    "  \"EffectiveDate\": \"Payment Date\",\n",
    "  \"TermDate\": \"blank\",\n",
    "  \"PaySched\": \"Default to Monthly\",\n",
    "  \"PayCode\": \"Default to Default\",\n",
    "  \"WritingAgentID\": \"Writing Agent\",\n",
    "  \"Premium\": \"Commission\",\n",
    "  \"CommPrem\": \"Commission\",\n",
    "  \"TranDate\": \"Payment Date\",\n",
    "  \"CommReceived\": \"Commission\",\n",
    "  \"PID\": \"Paid To Date (plus 1 month)\",\n",
    "  \"NoPayMon\": \"Default to -1\",\n",
    "  \"Membercount\": \"Default to 1 unless Commission is negative then -1\",\n",
    "  \"Note\": \"blank\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 7a) Build LLM\n",
    "llm = build_llm(temperature=1.0)\n",
    "\n",
    "# 7b) Compile rule spec WITH THE LLM (this is the key part you wanted)\n",
    "rule_spec_raw = derive_rule_spec(\n",
    "    llm=llm,\n",
    "    required_fields=FINAL_COLUMNS,\n",
    "    headers=list(df_flat.columns),\n",
    "    rules_text=RULES_TEXT\n",
    ")\n",
    "\n",
    "# 7c) Bind sources to actual flattened headers (fixes small name variance)\n",
    "rule_spec = bind_rule_sources_to_headers(df_flat, rule_spec_raw)\n",
    "\n",
    "# Optional: audit any stragglers\n",
    "misses = audit_missing_sources(df_flat, rule_spec)\n",
    "for tgt, op, src, sugg in misses:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) '{src}'  suggestions: {sugg}\")\n",
    "\n",
    "# 7d) Apply rules vectorized\n",
    "out_df = apply_rules_vectorized(df_flat, rule_spec)\n",
    "\n",
    "# 7e) Save outputs\n",
    "OUT_DIR = Path(\"/mnt/data/outbound\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(out_df.to_csv(OUT_DIR / \"manhattan_life_standard.csv\", index=False))\n",
    "(Path(OUT_DIR / \"manhattan_life_standard.json\")\n",
    " .write_text(out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Done. Wrote:\")\n",
    "print((OUT_DIR / \"manhattan_life_standard.csv\").as_posix())\n",
    "print((OUT_DIR / \"manhattan_life_standard.json\").as_posix())\n",
    "print(out_df.head().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40380871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3282ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d33d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Azure OpenAI config (EDIT)\n",
    "# =========================\n",
    "import os\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"<YOUR_API_KEY>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://<your-resource>.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-4o-mini\")     # your chat deployment name\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import AzureChatOpenAI   # older langchain fallback\n",
    "\n",
    "# =========================\n",
    "# Constants / Schema\n",
    "# =========================\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "    \"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Output JSON ONLY, no prose.\n",
    "Return a mapping where keys are required output fields and values are objects:\n",
    "{ \"op\": <one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,money,membercount_from_commission,blank]>,\n",
    "  \"source\": <column name when applicable>,\n",
    "  \"value\": <for const> }.\n",
    "If a rule says 'TBD' or 'blank' or is unclear, use {\"op\":\"blank\"}.\n",
    "Never invent columns. Use exact source header strings when copying.\n",
    "Use 'PID' if rules refer to PID but output needs PTD.\n",
    "Return STRICT JSON only. No prose.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# Carrier registry (EDIT paths)\n",
    "# =========================\n",
    "CARRIERS = {\n",
    "    \"molina\": {\n",
    "        \"prompt_path\": \"./carrier_prompts/molina_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/molina_rules.json\",      # narrative rules text for LLM\n",
    "        \"raw_path\":    \"./data/molina_raw_data.csv\",\n",
    "        \"loader\":      \"csv\"                                       # one-row header\n",
    "    },\n",
    "    \"ameritas\": {\n",
    "        \"prompt_path\": \"./carrier_prompts/ameritas_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/ameritas_rules.json\",\n",
    "        \"raw_path\":    \"./data/ameritas_raw_data.csv\",\n",
    "        \"loader\":      \"csv\"\n",
    "    },\n",
    "    \"manhattan_life\": {\n",
    "        \"prompt_path\": \"./carrier_prompts/manhattan_life_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/manhattan_life_rules.json\",\n",
    "        \"raw_path\":    \"/mnt/data/manhattan_life_raw_data.csv\",    # uploaded file path\n",
    "        \"loader\":      \"two_header\"                                 # uses the flattener\n",
    "    },\n",
    "    # add more carriers here...\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Azure LLM builder\n",
    "# =========================\n",
    "def build_llm(timeout: int = 20, temperature: float = 1.0) -> AzureChatOpenAI:\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "    if not (api_key and endpoint and deployment):\n",
    "        raise RuntimeError(\"Missing AZURE_OPENAI_* env vars\")\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,      # many Azure deployments allow only 1.0\n",
    "        request_timeout=timeout,\n",
    "        max_retries=0                 # keep latency predictable\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# CSV loaders\n",
    "# =========================\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "\n",
    "    # forward-fill top header row\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x:\n",
    "            last = x\n",
    "        ff.append(last)\n",
    "\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b: name = \"unnamed\"\n",
    "        elif not a:         name = b\n",
    "        elif not b:         name = a\n",
    "        else:               name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def load_carrier_data(issuer: str):\n",
    "    key = (issuer or \"\").strip().lower()\n",
    "    if key not in CARRIERS:\n",
    "        raise ValueError(f\"Unsupported issuer '{issuer}'. Known: {list(CARRIERS)}\")\n",
    "    cfg = CARRIERS[key]\n",
    "    prompt_text = Path(cfg[\"prompt_path\"]).read_text(encoding=\"utf-8\")\n",
    "    rules_text  = Path(cfg[\"rules_path\"]).read_text(encoding=\"utf-8\")\n",
    "    raw_path    = Path(cfg[\"raw_path\"]).as_posix()\n",
    "    if cfg[\"loader\"] == \"two_header\":\n",
    "        raw_df = flatten_two_header_csv(raw_path)\n",
    "    elif cfg[\"loader\"] == \"csv\":\n",
    "        raw_df = pd.read_csv(raw_path, dtype=str).fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loader '{cfg['loader']}' for {issuer}\")\n",
    "    return prompt_text, rules_text, raw_df\n",
    "\n",
    "# =========================\n",
    "# Robust header lookup\n",
    "# =========================\n",
    "def _build_header_index(df: pd.DataFrame) -> dict[str, str]:\n",
    "    idx: dict[str, str] = {}\n",
    "    for h in df.columns:\n",
    "        k = re.sub(r'[^a-z0-9]', '', str(h).lower())\n",
    "        idx[k] = h\n",
    "    return idx\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: dict[str, str], name: str) -> pd.Series:\n",
    "    if not name:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "    if name in df.columns:\n",
    "        return df[name].astype(str)\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower():\n",
    "            return df[h].astype(str)\n",
    "    key = re.sub(r'[^a-z0-9]', '', name.lower())\n",
    "    if key in header_index:\n",
    "        return df[header_index[key]].astype(str)\n",
    "    return pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# Vectorized helpers\n",
    "# =========================\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name: return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2: return \" \".join(parts[1:] + parts[:1])\n",
    "        return name\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    return num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# Spec normalizer / binder / auditor\n",
    "# =========================\n",
    "def normalize_rule_spec(rule_spec_in: dict) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for k, v in rule_spec_in.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            sv = v.strip()\n",
    "            if sv.lower() in (\"blank\",\"tbd\"):\n",
    "                out[k] = {\"op\":\"blank\"}\n",
    "            else:\n",
    "                out[k] = {\"op\":\"const\",\"value\":sv}\n",
    "        else:\n",
    "            out[k] = {\"op\":\"blank\"}\n",
    "    return out\n",
    "\n",
    "def _needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "        \"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def bind_rule_sources_to_headers(df: pd.DataFrame, rule_spec_in: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    rule_spec = normalize_rule_spec(rule_spec_in)\n",
    "    headers = list(df.columns)\n",
    "    norm_map = {re.sub(r'[^a-z0-9]','',h.lower()): h for h in headers}\n",
    "    fixed: Dict[str, Dict[str, Any]] = {}\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        spec = dict(spec)\n",
    "        op = str(spec.get(\"op\",\"\")).strip()\n",
    "        if op not in ALLOWED_OPS:\n",
    "            spec = {\"op\":\"blank\"}; fixed[tgt] = spec; continue\n",
    "        if not _needs_source(op):\n",
    "            fixed[tgt] = spec; continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            fixed[tgt] = spec; continue\n",
    "        cand = next((h for h in headers if h == src or h.lower()==src.lower()), None)\n",
    "        if cand is None:\n",
    "            key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "            cand = norm_map.get(key)\n",
    "        if cand is None:\n",
    "            raw = get_close_matches(src, headers, n=1)\n",
    "            if raw: cand = raw[0]\n",
    "            else:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                norm = get_close_matches(key, list(norm_map.keys()), n=1)\n",
    "                if norm: cand = norm_map[norm[0]]\n",
    "        if cand: spec[\"source\"] = cand\n",
    "        fixed[tgt] = spec\n",
    "    return fixed\n",
    "\n",
    "def audit_missing_sources(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]):\n",
    "    headers = list(df.columns)\n",
    "    norm = {re.sub(r'[^a-z0-9]','',h.lower()):h for h in headers}\n",
    "    misses = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        op = str(spec.get(\"op\",\"\"))\n",
    "        if not _needs_source(op): continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            misses.append((tgt, op, src, [])); continue\n",
    "        ok = (src in headers) or any(h.lower()==src.lower() for h in headers) \\\n",
    "             or (re.sub(r'[^a-z0-9]','',src.lower()) in norm)\n",
    "        if not ok:\n",
    "            sugg = get_close_matches(src, headers, n=3)\n",
    "            if not sugg:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                sugg = [norm[k] for k in get_close_matches(key, list(norm.keys()), n=3)]\n",
    "            misses.append((tgt, op, src, sugg))\n",
    "    return misses\n",
    "\n",
    "# =========================\n",
    "# Executor\n",
    "# =========================\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec_in: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rule_spec = normalize_rule_spec(rule_spec_in)\n",
    "    header_index = _build_header_index(df)\n",
    "    def empty() -> pd.Series: return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not isinstance(spec, dict): out[tgt] = empty(); continue\n",
    "        op = str(spec.get(\"op\",\"\")).strip()\n",
    "        if op not in ALLOWED_OPS: out[tgt] = empty(); continue\n",
    "        if   op == \"copy\":                  out[tgt] = _get_col(df, header_index, spec.get(\"source\",\"\"))\n",
    "        elif op == \"const\":                 out[tgt] = pd.Series([str(spec.get(\"value\",\"\"))]*len(df), index=df.index, dtype=\"string\")\n",
    "        elif op == \"date_mmddyyyy\":         out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\": out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"name_first_from_full\":  out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[0]\n",
    "        elif op == \"name_last_from_full\":   out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[1]\n",
    "        elif op == \"money\":                 out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif op == \"blank\":                 out[tgt] = empty()\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# =========================\n",
    "# LLM compile → bind → apply (driven by issuer flag)\n",
    "# =========================\n",
    "issuer = \"manhattan_life\"   # <-- set this to 'molina' | 'ameritas' | 'manhattan_life' | etc.\n",
    "\n",
    "# 1) Load external data based on issuer\n",
    "prompt_text, RULES_TEXT, raw_df = load_carrier_data(issuer)\n",
    "print(f\"Loaded issuer='{issuer}', raw shape={raw_df.shape}\")\n",
    "\n",
    "# 2) Build the LLM and compile the operational rule spec from the narrative rules\n",
    "llm = build_llm(temperature=1.0)\n",
    "payload = {\n",
    "    \"RequiredFields\": FINAL_COLUMNS,\n",
    "    \"RawHeaders\": list(raw_df.columns),\n",
    "    \"RulesNarrative\": RULES_TEXT,\n",
    "    \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "}\n",
    "resp = llm.invoke([\n",
    "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "    {\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "])\n",
    "\n",
    "try:\n",
    "    compiled_rule_spec = json.loads(resp.content if hasattr(resp, \"content\") else str(resp))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"LLM did not return valid JSON.\\nRaw output:\\n{resp}\") from e\n",
    "\n",
    "# (optional) persist compiled spec for audit/reuse\n",
    "compiled_out = Path(f\"./carrier_prompts/{issuer}_compiled_rules.json\")\n",
    "compiled_out.write_text(json.dumps(compiled_rule_spec, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Compiled rule spec saved → {compiled_out.as_posix()}\")\n",
    "\n",
    "# 3) Bind LLM sources to actual headers (handles spaces/_/case/fuzzy)\n",
    "bound_spec = bind_rule_sources_to_headers(raw_df, compiled_rule_spec)\n",
    "\n",
    "# 4) Audit any unresolved sources\n",
    "misses = audit_missing_sources(raw_df, bound_spec)\n",
    "for tgt, op, src, sugg in misses:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) '{src}'  suggestions: {sugg}\")\n",
    "\n",
    "# 5) Apply the transform → standard template\n",
    "out_df = apply_rules_vectorized(raw_df, bound_spec)\n",
    "\n",
    "# 6) Save outputs\n",
    "OUT_DIR = Path(\"/mnt/data/outbound\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_csv = OUT_DIR / f\"{issuer}_standard.csv\"\n",
    "out_json = OUT_DIR / f\"{issuer}_standard.json\"\n",
    "out_df.to_csv(out_csv, index=False)\n",
    "out_json.write_text(out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(out_csv.as_posix())\n",
    "print(out_json.as_posix())\n",
    "display(out_df.head().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eaee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9ab98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Azure OpenAI config (EDIT)\n",
    "# =========================\n",
    "import os\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"<YOUR_API_KEY>\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://<your-resource>.openai.azure.com/\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-4o-mini\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# =========================\n",
    "# Constants / Schema\n",
    "# =========================\n",
    "FINAL_COLUMNS = [\n",
    "    \"PolicyNo\",\"PHFirst\",\"PHLast\",\"Status\",\"Issuer\",\"State\",\"ProductType\",\"PlanName\",\n",
    "    \"SubmittedDate\",\"EffectiveDate\",\"TermDate\",\"PaySched\",\"PayCode\",\"WritingAgentID\",\n",
    "    \"Premium\",\"CommPrem\",\"TranDate\",\"CommReceived\",\"PTD\",\"NoPayMon\",\"Membercount\"\n",
    "]\n",
    "ALLOWED_OPS = {\n",
    "    \"copy\",\"const\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "    \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "    \"membercount_from_commission\",\"blank\"\n",
    "}\n",
    "SYSTEM_PROMPT = \"\"\"You are a data transformation agent.\n",
    "Output JSON ONLY, no prose.\n",
    "Return a mapping where keys are required output fields and values are objects:\n",
    "{ \"op\": <one of [copy,const,date_mmddyyyy,date_plus_1m_mmddyyyy,name_first_from_full,name_last_from_full,money,membercount_from_commission,blank]>,\n",
    "  \"source\": <column name when applicable>,\n",
    "  \"value\": <for const> }.\n",
    "If a rule says 'TBD' or 'blank' or is unclear, use {\"op\":\"blank\"}.\n",
    "Never invent columns. Use exact source header strings when copying.\n",
    "Use 'PID' if rules refer to PID but output needs PTD.\n",
    "Return STRICT JSON only. No prose.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# Carrier registry (Manhattan Life = two-row flatten)\n",
    "# =========================\n",
    "CARRIERS = {\n",
    "    \"molina\": {\n",
    "        \"prompt_path\": \"./carrier_prompts/molina_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/molina_rules.json\",\n",
    "        \"raw_path\":    \"./data/molina_raw_data.csv\",\n",
    "        \"loader\":      \"csv\"            # one-row header\n",
    "    },\n",
    "    \"ameritas\": {\n",
    "        \"prompt_path\": \"./carrier_prompts/ameritas_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/ameritas_rules.json\",\n",
    "        \"raw_path\":    \"./data/ameritas_raw_data.csv\",\n",
    "        \"loader\":      \"csv\"            # one-row header\n",
    "    },\n",
    "    \"manhattan_life\": {                 # <-- spelling\n",
    "        \"prompt_path\": \"./carrier_prompts/manhattan_life_prompt.txt\",\n",
    "        \"rules_path\":  \"./carrier_prompts/manhattan_life_rules.json\",\n",
    "        \"raw_path\":    \"/mnt/data/manhattan_life_raw_data.csv\",\n",
    "        \"loader\":      \"two_header\"     # <-- uses flattener ONLY for this carrier\n",
    "    },\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# LLM builder\n",
    "# =========================\n",
    "def build_llm(timeout: int = 20, temperature: float = 1.0) -> AzureChatOpenAI:\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "    if not (api_key and endpoint and deployment):\n",
    "        raise RuntimeError(\"Missing AZURE_OPENAI_* env vars\")\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "        max_retries=0\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Loaders\n",
    "# =========================\n",
    "def flatten_two_header_csv(path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(path, header=None, dtype=str).fillna(\"\")\n",
    "    top, bottom = tmp.iloc[0].tolist(), tmp.iloc[1].tolist()\n",
    "    ff, last = [], \"\"\n",
    "    for x in top:\n",
    "        x = str(x).strip()\n",
    "        if x: last = x\n",
    "        ff.append(last)\n",
    "    cols = []\n",
    "    for a, b in zip(ff, bottom):\n",
    "        a, b = str(a).strip(), str(b).strip()\n",
    "        if not a and not b: name = \"unnamed\"\n",
    "        elif not a:         name = b\n",
    "        elif not b:         name = a\n",
    "        else:               name = f\"{a} {b}\"\n",
    "        name = re.sub(r\"\\s+\", \" \", name)\n",
    "        name = name.replace(\"/\", \"_\").replace(\".\", \"_\").strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        cols.append(name)\n",
    "    df = tmp.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = cols\n",
    "    df = df[[c for c in df.columns if not df[c].astype(str).str.strip().eq(\"\").all()]]\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "def load_carrier_data(issuer: str):\n",
    "    key = (issuer or \"\").strip().lower()\n",
    "    if key not in CARRIERS:\n",
    "        raise ValueError(f\"Unsupported issuer '{issuer}'. Known: {list(CARRIERS)}\")\n",
    "    cfg = CARRIERS[key]\n",
    "    prompt_text = Path(cfg[\"prompt_path\"]).read_text(encoding=\"utf-8\")\n",
    "    rules_text  = Path(cfg[\"rules_path\"]).read_text(encoding=\"utf-8\")\n",
    "    raw_path    = Path(cfg[\"raw_path\"]).as_posix()\n",
    "    if cfg[\"loader\"] == \"two_header\":\n",
    "        raw_df = flatten_two_header_csv(raw_path)\n",
    "    elif cfg[\"loader\"] == \"csv\":\n",
    "        raw_df = pd.read_csv(raw_path, dtype=str).fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loader '{cfg['loader']}' for {issuer}\")\n",
    "    return prompt_text, rules_text, raw_df\n",
    "\n",
    "# =========================\n",
    "# Header utilities\n",
    "# =========================\n",
    "def _build_header_index(df: pd.DataFrame) -> dict[str, str]:\n",
    "    return {re.sub(r'[^a-z0-9]','',str(h).lower()): h for h in df.columns}\n",
    "\n",
    "def _get_col(df: pd.DataFrame, header_index: dict[str, str], name: str) -> pd.Series:\n",
    "    if not name: return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    if name in df.columns: return df[name].astype(str)\n",
    "    for h in df.columns:\n",
    "        if h.lower() == name.lower(): return df[h].astype(str)\n",
    "    key = re.sub(r'[^a-z0-9]','',name.lower())\n",
    "    if key in header_index: return df[header_index[key]].astype(str)\n",
    "    return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# Vectorized helpers\n",
    "# =========================\n",
    "def _to_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt.dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _add_one_month_mmddyyyy(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    dtp = dt.apply(lambda x: x + relativedelta(months=1) if pd.notnull(x) else pd.NaT)\n",
    "    return pd.Series(dtp).dt.strftime(\"%m/%d/%Y\").fillna(\"\").astype(\"string\")\n",
    "\n",
    "def _parse_case_name_first_last(series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    comma = s.str.contains(\",\", regex=False)\n",
    "    swapped = s.where(~comma, s.str.replace(\",\", \"\", regex=False).str.strip())\n",
    "    def _normalize(name: str) -> str:\n",
    "        if not name: return \"\"\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2: return \" \".join(parts[1:] + parts[:1])\n",
    "        return name\n",
    "    normalized = swapped.where(~comma, swapped.map(_normalize))\n",
    "    tokens = normalized.str.split()\n",
    "    last = tokens.str[-1].fillna(\"\")\n",
    "    first = tokens.apply(lambda xs: \" \".join(xs[:-1]) if isinstance(xs, list) and len(xs) > 1 else \"\").fillna(\"\")\n",
    "    return first.str.title().astype(\"string\"), last.str.title().astype(\"string\")\n",
    "\n",
    "def _money_to_float_str(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    return num.map(lambda v: f\"{v:.2f}\" if pd.notnull(v) else \"\")\n",
    "\n",
    "def _sign_flag_from_money(s: pd.Series) -> pd.Series:\n",
    "    x = s.fillna(\"\").astype(str).str.strip()\n",
    "    neg_paren = x.str.match(r\"^\\(.*\\)$\")\n",
    "    x = x.str.replace(r\"[\\$,()]\", \"\", regex=True).str.strip()\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    num = num.where(~neg_paren, -num)\n",
    "    out = np.where(pd.isna(num), \"\", np.where(num < 0, \"-1\", \"1\"))\n",
    "    return pd.Series(out, index=s.index, dtype=\"string\")\n",
    "\n",
    "# =========================\n",
    "# Spec normalizer / binder / auditor\n",
    "# =========================\n",
    "def normalize_rule_spec(rule_spec_in: dict) -> Dict[str, Dict[str, Any]]:\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    for k, v in rule_spec_in.items():\n",
    "        if isinstance(v, dict):\n",
    "            out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            sv = v.strip()\n",
    "            if sv.lower() in (\"blank\",\"tbd\"): out[k] = {\"op\":\"blank\"}\n",
    "            else: out[k] = {\"op\":\"const\",\"value\":sv}\n",
    "        else:\n",
    "            out[k] = {\"op\":\"blank\"}\n",
    "    return out\n",
    "\n",
    "def _needs_source(op: str) -> bool:\n",
    "    return op in {\n",
    "        \"copy\",\"date_mmddyyyy\",\"date_plus_1m_mmddyyyy\",\n",
    "        \"name_first_from_full\",\"name_last_from_full\",\"money\",\n",
    "        \"membercount_from_commission\"\n",
    "    }\n",
    "\n",
    "def bind_rule_sources_to_headers(df: pd.DataFrame, rule_spec_in: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    rule_spec = normalize_rule_spec(rule_spec_in)\n",
    "    headers = list(df.columns)\n",
    "    norm_map = {re.sub(r'[^a-z0-9]','',h.lower()): h for h in headers}\n",
    "    fixed: Dict[str, Dict[str, Any]] = {}\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        spec = dict(spec)\n",
    "        op = str(spec.get(\"op\",\"\")).strip()\n",
    "        if op not in ALLOWED_OPS: spec = {\"op\":\"blank\"}; fixed[tgt] = spec; continue\n",
    "        if not _needs_source(op): fixed[tgt] = spec; continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src: fixed[tgt] = spec; continue\n",
    "        cand = next((h for h in headers if h == src or h.lower()==src.lower()), None)\n",
    "        if cand is None:\n",
    "            key = re.sub(r'[^a-z0-9]','',src.lower()); cand = norm_map.get(key)\n",
    "        if cand is None:\n",
    "            raw = get_close_matches(src, headers, n=1)\n",
    "            if raw: cand = raw[0]\n",
    "            else:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                norm = get_close_matches(key, list(norm_map.keys()), n=1)\n",
    "                if norm: cand = norm_map[norm[0]]\n",
    "        if cand: spec[\"source\"] = cand\n",
    "        fixed[tgt] = spec\n",
    "    return fixed\n",
    "\n",
    "def audit_missing_sources(df: pd.DataFrame, rule_spec: Dict[str, Dict[str, Any]]):\n",
    "    headers = list(df.columns)\n",
    "    norm = {re.sub(r'[^a-z0-9]','',h.lower()):h for h in headers}\n",
    "    misses = []\n",
    "    for tgt, spec in rule_spec.items():\n",
    "        op = str(spec.get(\"op\",\"\"))\n",
    "        if not _needs_source(op): continue\n",
    "        src = (spec.get(\"source\") or \"\").strip()\n",
    "        if not src:\n",
    "            misses.append((tgt, op, src, [])); continue\n",
    "        ok = (src in headers) or any(h.lower()==src.lower() for h in headers) \\\n",
    "             or (re.sub(r'[^a-z0-9]','',src.lower()) in norm)\n",
    "        if not ok:\n",
    "            sugg = get_close_matches(src, headers, n=3)\n",
    "            if not sugg:\n",
    "                key = re.sub(r'[^a-z0-9]','',src.lower())\n",
    "                sugg = [norm[k] for k in get_close_matches(key, list(norm.keys()), n=3)]\n",
    "            misses.append((tgt, op, src, sugg))\n",
    "    return misses\n",
    "\n",
    "# =========================\n",
    "# Executor\n",
    "# =========================\n",
    "def apply_rules_vectorized(df: pd.DataFrame, rule_spec_in: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rule_spec = normalize_rule_spec(rule_spec_in)\n",
    "    header_index = _build_header_index(df)\n",
    "    def empty() -> pd.Series: return pd.Series([\"\"]*len(df), index=df.index, dtype=\"string\")\n",
    "    out: Dict[str, pd.Series] = {}\n",
    "    for tgt in FINAL_COLUMNS:\n",
    "        spec = rule_spec.get(tgt) or (rule_spec.get(\"PID\") if tgt == \"PTD\" else None)\n",
    "        if not isinstance(spec, dict): out[tgt] = empty(); continue\n",
    "        op = str(spec.get(\"op\",\"\")).strip()\n",
    "        if op not in ALLOWED_OPS: out[tgt] = empty(); continue\n",
    "        if   op == \"copy\":                  out[tgt] = _get_col(df, header_index, spec.get(\"source\",\"\"))\n",
    "        elif op == \"const\":                 out[tgt] = pd.Series([str(spec.get(\"value\",\"\"))]*len(df), index=df.index, dtype=\"string\")\n",
    "        elif op == \"date_mmddyyyy\":         out[tgt] = _to_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"date_plus_1m_mmddyyyy\": out[tgt] = _add_one_month_mmddyyyy(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"name_first_from_full\":  out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[0]\n",
    "        elif op == \"name_last_from_full\":   out[tgt] = _parse_case_name_first_last(_get_col(df, header_index, spec.get(\"source\",\"\")))[1]\n",
    "        elif op == \"money\":                 out[tgt] = _money_to_float_str(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "        elif op == \"membercount_from_commission\":\n",
    "            flags = _sign_flag_from_money(_get_col(df, header_index, spec.get(\"source\",\"\")))\n",
    "            out[tgt] = pd.Series(np.where(flags.eq(\"\"), \"1\", flags), index=df.index, dtype=\"string\")\n",
    "        elif op == \"blank\":                 out[tgt] = empty()\n",
    "    return pd.DataFrame(out, columns=FINAL_COLUMNS).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# =========================\n",
    "# RUN: choose issuer, LLM compile → bind → apply\n",
    "# =========================\n",
    "issuer = \"manhattan_life\"   # <-- ONLY this issuer triggers flatten_two_header_csv\n",
    "prompt_text, RULES_TEXT, raw_df = load_carrier_data(issuer)\n",
    "print(f\"Loaded issuer='{issuer}', raw shape={raw_df.shape}\")\n",
    "\n",
    "llm = build_llm(temperature=1.0)\n",
    "payload = {\n",
    "    \"RequiredFields\": FINAL_COLUMNS,\n",
    "    \"RawHeaders\": list(raw_df.columns),\n",
    "    \"RulesNarrative\": RULES_TEXT,\n",
    "    \"OutputFormat\": \"Return STRICT JSON object keyed by RequiredFields (use 'PID' if rules say PID but output needs PTD). No prose.\"\n",
    "}\n",
    "resp = llm.invoke([\n",
    "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "    {\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}\n",
    "])\n",
    "\n",
    "try:\n",
    "    compiled_rule_spec = json.loads(resp.content if hasattr(resp, \"content\") else str(resp))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"LLM did not return valid JSON.\\nRaw output:\\n{resp}\") from e\n",
    "\n",
    "compiled_out = Path(f\"./carrier_prompts/{issuer}_compiled_rules.json\")\n",
    "compiled_out.write_text(json.dumps(compiled_rule_spec, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Compiled rule spec saved → {compiled_out.as_posix()}\")\n",
    "\n",
    "bound_spec = bind_rule_sources_to_headers(raw_df, compiled_rule_spec)\n",
    "\n",
    "misses = audit_missing_sources(raw_df, bound_spec)\n",
    "for tgt, op, src, sugg in misses:\n",
    "    print(f\"[MISSING] {tgt} <- ({op}) '{src}'  suggestions: {sugg}\")\n",
    "\n",
    "out_df = apply_rules_vectorized(raw_df, bound_spec)\n",
    "\n",
    "OUT_DIR = Path(\"/mnt/data/outbound\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_csv = OUT_DIR / f\"{issuer}_standard.csv\"\n",
    "out_json = OUT_DIR / f\"{issuer}_standard.json\"\n",
    "out_df.to_csv(out_csv, index=False)\n",
    "out_json.write_text(out_df.to_json(orient=\"records\", force_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(out_csv.as_posix())\n",
    "print(out_json.as_posix())\n",
    "display(out_df.head().T)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
